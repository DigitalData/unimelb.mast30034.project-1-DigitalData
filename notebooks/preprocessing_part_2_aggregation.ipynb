{"cells":[{"cell_type":"markdown","metadata":{},"source":["### MAST30034: Applied Data Science Project 1\n","---\n","# Preprocessing Part 2: Aggregating Data by MMWR Week\n","#### Xavier Travers (1178369)\n","\n","Aggregate all the data by MMWR week (defined [here](https://ndc.services.cdc.gov/wp-content/uploads/MMWR_Week_overview.pdf)).\n","This means counting trips to and from each of the boroughs per month.\n","This is done for each of the taxi types."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# imports used throughout this notebook\n","from pyspark.sql import DataFrame\n","from pyspark.sql import functions as F\n","import os\n","import sys\n","import re\n","from itertools import chain\n","\n","# add homemade helpers\n","sys.path.insert(1, '../scripts')\n","import helpers.aggregation_helpers as ah\n","import helpers.join_helpers as jh\n","\n","# for printouts\n","DEBUGGING = True"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["22/08/11 23:57:32 WARN Utils: Your hostname, Polaris resolves to a loopback address: 127.0.1.1; using 172.20.85.243 instead (on interface eth0)\n","22/08/11 23:57:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"]},{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"]},{"name":"stdout","output_type":"stream","text":["22/08/11 23:57:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","22/08/11 23:57:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n","22/08/11 23:57:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","# Create a spark session (which will run spark jobs)\n","spark = (\n","    SparkSession.builder.appName('MAST30034 XT Project 1')\n","    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n","    .config('spark.sql.repl.eagerEval.enabled', True) \n","    .config('spark.sql.parquet.cacheMetadata', 'true')\n","    .config(\"spark.executor.memory\", \"2g\")\n","    .config(\"spark.driver.memory\", \"4g\")\n","    .getOrCreate()\n",")"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":["<table border='1'>\n","<tr><th>OBJECTID</th><th>Shape_Leng</th><th>the_geom</th><th>Shape_Area</th><th>zone</th><th>LocationID</th><th>borough</th></tr>\n","<tr><td>1</td><td>0.116357453189</td><td>MULTIPOLYGON (((-...</td><td>0.0007823067885</td><td>Newark Airport</td><td>1</td><td>EWR</td></tr>\n","<tr><td>2</td><td>0.43346966679</td><td>MULTIPOLYGON (((-...</td><td>0.00486634037837</td><td>Jamaica Bay</td><td>2</td><td>Queens</td></tr>\n","<tr><td>3</td><td>0.0843411059012</td><td>MULTIPOLYGON (((-...</td><td>0.000314414156821</td><td>Allerton/Pelham G...</td><td>3</td><td>Bronx</td></tr>\n","<tr><td>4</td><td>0.0435665270921</td><td>MULTIPOLYGON (((-...</td><td>0.000111871946192</td><td>Alphabet City</td><td>4</td><td>Manhattan</td></tr>\n","<tr><td>5</td><td>0.0921464898574</td><td>MULTIPOLYGON (((-...</td><td>0.000497957489363</td><td>Arden Heights</td><td>5</td><td>Staten Island</td></tr>\n","</table>\n"],"text/plain":["+--------+---------------+--------------------+-----------------+--------------------+----------+-------------+\n","|OBJECTID|     Shape_Leng|            the_geom|       Shape_Area|                zone|LocationID|      borough|\n","+--------+---------------+--------------------+-----------------+--------------------+----------+-------------+\n","|       1| 0.116357453189|MULTIPOLYGON (((-...|  0.0007823067885|      Newark Airport|         1|          EWR|\n","|       2|  0.43346966679|MULTIPOLYGON (((-...| 0.00486634037837|         Jamaica Bay|         2|       Queens|\n","|       3|0.0843411059012|MULTIPOLYGON (((-...|0.000314414156821|Allerton/Pelham G...|         3|        Bronx|\n","|       4|0.0435665270921|MULTIPOLYGON (((-...|0.000111871946192|       Alphabet City|         4|    Manhattan|\n","|       5|0.0921464898574|MULTIPOLYGON (((-...|0.000497957489363|       Arden Heights|         5|Staten Island|\n","+--------+---------------+--------------------+-----------------+--------------------+----------+-------------+"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# read in the taxi zones dataset\n","zones_df = spark.read.csv('../data/raw/tlc_zones/zones.csv',\n","    header = True)\n","zones_df.limit(5)"]},{"cell_type":"markdown","metadata":{},"source":["### 1. Aggregating the TLC dataset"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# TODO: commenting\n","TLC_NAMES = ['yellow']"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../data/curated/tlc/cleaned/yellow'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/mnt/c/Users/Xavier Travers/Desktop/Uni/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_2_aggregation.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Xavier%20Travers/Desktop/Uni/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_2_aggregation.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# read in the yellow dataset# TODO: commenting\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Xavier%20Travers/Desktop/Uni/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_2_aggregation.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m tlc_df \u001b[39m=\u001b[39m jh\u001b[39m.\u001b[39;49mstacked_tlc_df(spark, \u001b[39m'\u001b[39;49m\u001b[39m../data/curated/tlc/cleaned/yellow\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Xavier%20Travers/Desktop/Uni/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_2_aggregation.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mtlc_df\u001b[39m.\u001b[39mcount()\u001b[39m}\u001b[39;00m\u001b[39m ROWS\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Xavier%20Travers/Desktop/Uni/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_2_aggregation.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(tlc_df\u001b[39m.\u001b[39mlimit(\u001b[39m5\u001b[39m))\n","File \u001b[0;32m/mnt/c/Users/Xavier Travers/Desktop/Uni/ADS/mast30034-project-1-DigitalData/notebooks/../scripts/helpers/join_helpers.py:40\u001b[0m, in \u001b[0;36mstacked_tlc_df\u001b[0;34m(spark, location)\u001b[0m\n\u001b[1;32m     37\u001b[0m stacked_df \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m# iterate through the downloaded files per taxi type\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(location):\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m     \u001b[39m# read the parquet in\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     tlc_df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mparquet(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mlocation\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m stacked_df \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m:\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/curated/tlc/cleaned/yellow'"]}],"source":["# read in the yellow dataset# TODO: commenting\n","tlc_df = jh.read_stacked_tlc_df(spark, '../data/curated/tlc/cleaned/yellow')\n","print(f'{tlc_df.count()} ROWS')\n","print(tlc_df.limit(5))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TLC_GROUP_COLUMNS = [\n","    'year',\n","    'cdc_week',\n","    'week_index',\n","    'pu_borough',\n","    'do_borough',\n","    'passengers'\n","]\n","# TODO: commenting\n","TLC_AGGREGATE_COLUMNS = {\n","    '*': ['count'],\n","    'trip_distance': ['total', 'average'],\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tlc_df = ah.group_and_aggregate(tlc_df, TLC_GROUP_COLUMNS, \n","    TLC_AGGREGATE_COLUMNS)\n","# TODO: commenting\n","# force this into memory \n","# otherwise writing parquets results in a java executor out of memory error\n","tlc_df = spark.createDataFrame(tlc_df.collect())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tlc_df.sort('week_index').limit(5)\n","# TODO: commenting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tlc_df.sort('avg_trip_distance', ascending=False).limit(20)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tlc_df.write.mode('overwrite').parquet('../data/curated/tlc/aggregated/yellow')\n","# TODO: commenting"]},{"cell_type":"markdown","metadata":{},"source":["### 2. Aggregating the COVID dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# read in the covid dataset\n","covid_df = spark.read.parquet('../data/curated/virals/covid/cases-by-day')\n","covid_df.limit(5)\n","# TODO: commenting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["COVID_GROUP_COLUMNS = [\n","    'year',\n","    'cdc_week',\n","    'week_index',\n","    'borough'\n","]\n","# TODO: commenting\n","COVID_AGGREGATE_COLUMNS = {\n","    'cases': ['total', 'average'],\n","    'deaths': ['total', 'average'],\n","    'hospitalised': ['total', 'average'],\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["covid_df = ah.group_and_aggregate(covid_df, COVID_GROUP_COLUMNS, \n","    COVID_AGGREGATE_COLUMNS)\n","\n","# force this into memory \n","# otherwise writing parquets results in a java executor out of memory error\n","covid_df = spark.createDataFrame(covid_df.collect())\n","# TODO: commenting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["covid_df.sort('week_index').limit(5)\n","# TODO: commenting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save it\n","# TODO: commenting\n","covid_df.write.mode('overwrite').parquet('../data/curated/virals/covid/cases-by-week')"]},{"cell_type":"markdown","metadata":{},"source":["### 3. Aggregating the Flu dataset\n","*Nothing is done here, since the flu dataset is already grouped by MMWR week.*"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":2}
