{"cells":[{"cell_type":"markdown","metadata":{},"source":["### MAST30034: Applied Data Science Project 1\n","---\n","# Preprocessing Part 2: Aggregating Data by MMWR Week\n","#### Xavier Travers (1178369)\n","\n","Aggregate all the data by MMWR week (defined [here](https://ndc.services.cdc.gov/wp-content/uploads/MMWR_Week_overview.pdf)).\n","This means counting trips to and from each of the boroughs per month.\n","This is done for each of the taxi types."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# imports used throughout this notebook\n","from pyspark.sql import DataFrame\n","from pyspark.sql import functions as F\n","import os\n","import sys\n","import re\n","from itertools import chain\n","\n","# add homemade helpers\n","sys.path.insert(1, '../scripts')\n","import helpers.aggregation_helpers as ah\n","import helpers.join_helpers as jh\n","\n","# Used for saving time (if you don't want sanity-check printouts)\n","INTERMEDIATE_OUTPUTS = False"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["22/08/13 18:53:46 WARN Utils: Your hostname, Polaris resolves to a loopback address: 127.0.1.1; using 172.26.235.73 instead (on interface eth0)\n","22/08/13 18:53:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"]},{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"]},{"name":"stdout","output_type":"stream","text":["22/08/13 18:53:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","22/08/13 18:53:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","# Create a spark session (which will run spark jobs)\n","spark = (\n","    SparkSession.builder.appName('MAST30034 XT Project 1')\n","    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n","    .config('spark.sql.repl.eagerEval.enabled', True) \n","    .config('spark.sql.parquet.cacheMetadata', 'true')\n","    .config(\"spark.executor.memory\", \"2g\")\n","    .config(\"spark.driver.memory\", \"4g\")\n","    .getOrCreate()\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### 1. Aggregating the TLC dataset"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# TODO: commenting\n","TLC_NAMES = ['yellow']"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# read in the cleaned yellow dataset\n","tlc_df = spark.read.parquet('../data/curated/tlc/cleaned/yellow')\n","\n","if INTERMEDIATE_OUTPUTS:\n","    print(f'{tlc_df.count()} ROWS')\n","    tlc_df.limit(5)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["TLC_COMMON_GROUP_COLUMNS = [\n","    'week_year',\n","    'week_month',\n","    'week_ending',\n","    'timeline'\n","]\n","\n","TLC_GROUP_BY_PU_COLUMNS = TLC_COMMON_GROUP_COLUMNS + ['pu_borough'];\n","TLC_GROUP_BY_DO_COLUMNS = TLC_COMMON_GROUP_COLUMNS + ['do_borough'];\n","\n","# TODO: commenting\n","TLC_AGGREGATE_COLUMNS = {\n","    '*': ['count'],\n","    'passengers': ['total', 'daily_average', 'average'],\n","    'trip_distance': ['total', 'daily_average', 'average'],\n","    'hours_elapsed': ['total', 'daily_average', 'average'],\n","}"]},{"cell_type":"markdown","metadata":{},"source":["#### Group by pick-up location"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["tlc_by_pu_df = ah.group_and_aggregate(tlc_df, TLC_GROUP_BY_PU_COLUMNS, \n","    TLC_AGGREGATE_COLUMNS)\n","# TODO: commenting\n","# force this into memory \n","# otherwise writing parquets results in a java executor out of memory error\n","tlc_by_pu_df = spark.createDataFrame(tlc_by_pu_df.collect())"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["if INTERMEDIATE_OUTPUTS:\n","    tlc_by_pu_df.sort('week_index').limit(5)\n","# TODO: commenting"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["if INTERMEDIATE_OUTPUTS:\n","    tlc_by_pu_df.sort('avg_trip_distance', ascending=False).limit(5)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["tlc_by_pu_df.write.mode('overwrite').parquet('../data/curated/tlc/aggregated/yellow/by_pu')\n","# TODO: commenting"]},{"cell_type":"markdown","metadata":{},"source":["#### Group by drop-off location"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["tlc_by_do_df = ah.group_and_aggregate(tlc_df, TLC_GROUP_BY_DO_COLUMNS, \n","    TLC_AGGREGATE_COLUMNS)\n","# TODO: commenting\n","# force this into memory \n","# otherwise writing parquets results in a java executor out of memory error\n","tlc_by_do_df = spark.createDataFrame(tlc_by_do_df.collect())"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["if INTERMEDIATE_OUTPUTS:\n","    tlc_by_do_df.sort('week_index').limit(5)\n","# TODO: commenting"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["if INTERMEDIATE_OUTPUTS:\n","    tlc_by_do_df.sort('avg_trip_distance', ascending=False).limit(5)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["tlc_by_do_df.write.mode('overwrite').parquet('../data/curated/tlc/aggregated/yellow/by_do')\n","# TODO: commenting"]},{"cell_type":"markdown","metadata":{},"source":["### 2. Aggregating the COVID dataset"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/html":["<table border='1'>\n","<tr><th>date</th><th>week_ending</th><th>week_year</th><th>week_month</th><th>timeline</th><th>cases</th><th>deaths</th><th>hospitalised</th><th>borough</th></tr>\n","<tr><td>02/29/2020</td><td>2020-02-29</td><td>2020</td><td>2</td><td>keep for graphing</td><td>0</td><td>0</td><td>1</td><td>Brooklyn</td></tr>\n","<tr><td>03/01/2020</td><td>2020-03-07</td><td>2020</td><td>3</td><td>post</td><td>0</td><td>0</td><td>0</td><td>Brooklyn</td></tr>\n","<tr><td>03/02/2020</td><td>2020-03-07</td><td>2020</td><td>3</td><td>post</td><td>0</td><td>0</td><td>2</td><td>Brooklyn</td></tr>\n","<tr><td>03/03/2020</td><td>2020-03-07</td><td>2020</td><td>3</td><td>post</td><td>0</td><td>0</td><td>3</td><td>Brooklyn</td></tr>\n","<tr><td>03/04/2020</td><td>2020-03-07</td><td>2020</td><td>3</td><td>post</td><td>1</td><td>0</td><td>1</td><td>Brooklyn</td></tr>\n","</table>\n"],"text/plain":["+----------+-----------+---------+----------+-----------------+-----+------+------------+--------+\n","|      date|week_ending|week_year|week_month|         timeline|cases|deaths|hospitalised| borough|\n","+----------+-----------+---------+----------+-----------------+-----+------+------------+--------+\n","|02/29/2020| 2020-02-29|     2020|         2|keep for graphing|    0|     0|           1|Brooklyn|\n","|03/01/2020| 2020-03-07|     2020|         3|             post|    0|     0|           0|Brooklyn|\n","|03/02/2020| 2020-03-07|     2020|         3|             post|    0|     0|           2|Brooklyn|\n","|03/03/2020| 2020-03-07|     2020|         3|             post|    0|     0|           3|Brooklyn|\n","|03/04/2020| 2020-03-07|     2020|         3|             post|    1|     0|           1|Brooklyn|\n","+----------+-----------+---------+----------+-----------------+-----+------+------------+--------+"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# read in the covid dataset\n","covid_df = spark.read.parquet('../data/curated/virals/covid/cleaned/cases-by-day')\n","covid_df.limit(5)\n","# TODO: commenting"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["COVID_GROUP_COLUMNS = [\n","    'week_year',\n","    'week_month',\n","    'week_ending',\n","    'timeline',\n","    'borough'\n","]\n","# TODO: commenting\n","COVID_AGGREGATE_COLUMNS = {\n","    'cases': ['total', 'daily_average'],\n","    'deaths': ['total', 'daily_average'],\n","    'hospitalised': ['total', 'daily_average'],\n","}"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["covid_df = ah.group_and_aggregate(covid_df, COVID_GROUP_COLUMNS, \n","    COVID_AGGREGATE_COLUMNS)\n","\n","# force this into memory \n","# otherwise writing parquets results in a java executor out of memory error\n","covid_df = spark.createDataFrame(covid_df.collect())\n","# TODO: commenting"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["if INTERMEDIATE_OUTPUTS:\n","    covid_df.sort('week_index').limit(5)\n","# TODO: commenting"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# save it\n","# TODO: commenting\n","covid_df.write.mode('overwrite').parquet('../data/curated/virals/covid/aggregated/cases-by-week')"]},{"cell_type":"markdown","metadata":{},"source":["### 3. Aggregating the Flu dataset\n","*The flu dataset is already grouped by MMWR week, so only daily_averages can be calculated*"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/html":["<table border='1'>\n","<tr><th>year</th><th>month</th><th>day</th><th>week_ending</th><th>week_year</th><th>week_month</th><th>timeline</th><th>date</th><th>borough</th><th>disease</th><th>cases</th></tr>\n","<tr><td>2018</td><td>1</td><td>20</td><td>2018-01-20</td><td>2018</td><td>1</td><td>neither</td><td>01/20/2018</td><td>Bronx</td><td>INFLUENZA_B</td><td>203</td></tr>\n","<tr><td>2018</td><td>5</td><td>5</td><td>2018-05-05</td><td>2018</td><td>5</td><td>pre</td><td>05/05/2018</td><td>Staten Island</td><td>INFLUENZA_UNSPECI...</td><td>0</td></tr>\n","<tr><td>2018</td><td>3</td><td>10</td><td>2018-03-10</td><td>2018</td><td>3</td><td>pre</td><td>03/10/2018</td><td>Manhattan</td><td>INFLUENZA_A</td><td>65</td></tr>\n","<tr><td>2018</td><td>3</td><td>3</td><td>2018-03-03</td><td>2018</td><td>2</td><td>neither</td><td>03/03/2018</td><td>Brooklyn</td><td>INFLUENZA_B</td><td>287</td></tr>\n","<tr><td>2018</td><td>5</td><td>12</td><td>2018-05-12</td><td>2018</td><td>5</td><td>pre</td><td>05/12/2018</td><td>Manhattan</td><td>INFLUENZA_B</td><td>10</td></tr>\n","</table>\n"],"text/plain":["+----+-----+---+-----------+---------+----------+--------+----------+-------------+--------------------+-----+\n","|year|month|day|week_ending|week_year|week_month|timeline|      date|      borough|             disease|cases|\n","+----+-----+---+-----------+---------+----------+--------+----------+-------------+--------------------+-----+\n","|2018|    1| 20| 2018-01-20|     2018|         1| neither|01/20/2018|        Bronx|         INFLUENZA_B|  203|\n","|2018|    5|  5| 2018-05-05|     2018|         5|     pre|05/05/2018|Staten Island|INFLUENZA_UNSPECI...|    0|\n","|2018|    3| 10| 2018-03-10|     2018|         3|     pre|03/10/2018|    Manhattan|         INFLUENZA_A|   65|\n","|2018|    3|  3| 2018-03-03|     2018|         2| neither|03/03/2018|     Brooklyn|         INFLUENZA_B|  287|\n","|2018|    5| 12| 2018-05-12|     2018|         5|     pre|05/12/2018|    Manhattan|         INFLUENZA_B|   10|\n","+----+-----+---+-----------+---------+----------+--------+----------+-------------+--------------------+-----+"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# read in the flu dataset\n","flu_df = spark.read.parquet('../data/curated/virals/flu/cleaned/cases-by-week')\n","flu_df.limit(5)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# add the daily average cases\n","flu_df = flu_df.withColumn(\n","    'daily_avg_cases',\n","    F.col('cases') / 7\n",")"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["flu_df.write.mode('overwrite').parquet('../data/curated/virals/flu/aggregated/cases-by-week')"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":2}
