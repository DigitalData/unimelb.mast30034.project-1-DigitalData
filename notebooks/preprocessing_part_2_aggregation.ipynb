{"cells":[{"cell_type":"markdown","metadata":{},"source":["### MAST30034: Applied Data Science Project 1\n","---\n","# Preprocessing Part 2: Aggregating Data by MMWR Week\n","#### Xavier Travers (1178369)\n","\n","Aggregate all the data by MMWR week (defined [here](https://ndc.services.cdc.gov/wp-content/uploads/MMWR_Week_overview.pdf)).\n","This means counting trips to and from each of the boroughs per month.\n","This is done for each of the taxi types."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# imports used throughout this notebook\n","from pyspark.sql import DataFrame\n","from pyspark.sql import functions as F\n","import os\n","import sys\n","import re\n","from itertools import chain\n","\n","# add homemade helpers\n","sys.path.insert(1, '../scripts')\n","import helpers.aggregation_helpers as ah\n","import helpers.join_helpers as jh\n","\n","# Used for saving time (if you don't want sanity-check printouts)\n","INTERMEDIATE_OUTPUTS = False"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["22/08/13 18:53:46 WARN Utils: Your hostname, Polaris resolves to a loopback address: 127.0.1.1; using 172.26.235.73 instead (on interface eth0)\n","22/08/13 18:53:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"]},{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"]},{"name":"stdout","output_type":"stream","text":["22/08/13 18:53:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","22/08/13 18:53:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","# Create a spark session (which will run spark jobs)\n","spark = (\n","    SparkSession.builder.appName('MAST30034 XT Project 1')\n","    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n","    .config('spark.sql.repl.eagerEval.enabled', True) \n","    .config('spark.sql.parquet.cacheMetadata', 'true')\n","    .config(\"spark.executor.memory\", \"2g\")\n","    .config(\"spark.driver.memory\", \"4g\")\n","    .getOrCreate()\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### 1. Aggregating the TLC dataset"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# TODO: commenting\n","TLC_NAMES = ['yellow']"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# read in the cleaned yellow dataset\n","tlc_df = spark.read.parquet('../data/curated/tlc/cleaned/yellow')\n","\n","if INTERMEDIATE_OUTPUTS:\n","    print(f'{tlc_df.count()} ROWS')\n","    tlc_df.limit(5)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["TLC_COMMON_GROUP_COLUMNS = [\n","    'week_year',\n","    'week_month',\n","    'week_ending',\n","    'week_index',\n","    'timeline'\n","]\n","\n","TLC_GROUP_BY_PU_COLUMNS = TLC_COMMON_GROUP_COLUMNS + ['pu_borough'];\n","TLC_GROUP_BY_DO_COLUMNS = TLC_COMMON_GROUP_COLUMNS + ['do_borough'];\n","\n","# TODO: commenting\n","TLC_AGGREGATE_COLUMNS = {\n","    '*': ['count'],\n","    'passengers': ['total', 'daily_average', 'average'],\n","    'trip_distance': ['total', 'daily_average', 'average'],\n","    'hours_elapsed': ['total', 'daily_average', 'average'],\n","}"]},{"cell_type":"markdown","metadata":{},"source":["#### Group by pick-up location"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["tlc_by_pu_df = ah.group_and_aggregate(tlc_df, TLC_GROUP_BY_PU_COLUMNS, \n","    TLC_AGGREGATE_COLUMNS)\n","# TODO: commenting\n","# force this into memory \n","# otherwise writing parquets results in a java executor out of memory error\n","tlc_by_pu_df = spark.createDataFrame(tlc_by_pu_df.collect())"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["if INTERMEDIATE_OUTPUTS:\n","    tlc_by_pu_df.sort('week_index').limit(5)\n","# TODO: commenting"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["if INTERMEDIATE_OUTPUTS:\n","    tlc_by_pu_df.sort('avg_trip_distance', ascending=False).limit(5)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["tlc_by_pu_df.write.mode('overwrite').parquet('../data/curated/tlc/aggregated/yellow/by_pu')\n","# TODO: commenting"]},{"cell_type":"markdown","metadata":{},"source":["#### Group by drop-off location"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["tlc_by_do_df = ah.group_and_aggregate(tlc_df, TLC_GROUP_BY_DO_COLUMNS, \n","    TLC_AGGREGATE_COLUMNS)\n","# TODO: commenting\n","# force this into memory \n","# otherwise writing parquets results in a java executor out of memory error\n","tlc_by_do_df = spark.createDataFrame(tlc_by_do_df.collect())"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["if INTERMEDIATE_OUTPUTS:\n","    tlc_by_do_df.sort('week_index').limit(5)\n","# TODO: commenting"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["if INTERMEDIATE_OUTPUTS:\n","    tlc_by_do_df.sort('avg_trip_distance', ascending=False).limit(5)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["tlc_by_do_df.write.mode('overwrite').parquet('../data/curated/tlc/aggregated/yellow/by_do')\n","# TODO: commenting"]},{"cell_type":"markdown","metadata":{},"source":["### 2. Aggregating the COVID dataset"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/html":["<table border='1'>\n","<tr><th>date</th><th>week_ending</th><th>week_year</th><th>week_month</th><th>timeline</th><th>cases</th><th>deaths</th><th>hospitalised</th><th>borough</th></tr>\n","<tr><td>02/29/2020</td><td>2020-02-29</td><td>2020</td><td>2</td><td>keep for graphing</td><td>0</td><td>0</td><td>1</td><td>Brooklyn</td></tr>\n","<tr><td>03/01/2020</td><td>2020-03-07</td><td>2020</td><td>3</td><td>post</td><td>0</td><td>0</td><td>0</td><td>Brooklyn</td></tr>\n","<tr><td>03/02/2020</td><td>2020-03-07</td><td>2020</td><td>3</td><td>post</td><td>0</td><td>0</td><td>2</td><td>Brooklyn</td></tr>\n","<tr><td>03/03/2020</td><td>2020-03-07</td><td>2020</td><td>3</td><td>post</td><td>0</td><td>0</td><td>3</td><td>Brooklyn</td></tr>\n","<tr><td>03/04/2020</td><td>2020-03-07</td><td>2020</td><td>3</td><td>post</td><td>1</td><td>0</td><td>1</td><td>Brooklyn</td></tr>\n","</table>\n"],"text/plain":["+----------+-----------+---------+----------+-----------------+-----+------+------------+--------+\n","|      date|week_ending|week_year|week_month|         timeline|cases|deaths|hospitalised| borough|\n","+----------+-----------+---------+----------+-----------------+-----+------+------------+--------+\n","|02/29/2020| 2020-02-29|     2020|         2|keep for graphing|    0|     0|           1|Brooklyn|\n","|03/01/2020| 2020-03-07|     2020|         3|             post|    0|     0|           0|Brooklyn|\n","|03/02/2020| 2020-03-07|     2020|         3|             post|    0|     0|           2|Brooklyn|\n","|03/03/2020| 2020-03-07|     2020|         3|             post|    0|     0|           3|Brooklyn|\n","|03/04/2020| 2020-03-07|     2020|         3|             post|    1|     0|           1|Brooklyn|\n","+----------+-----------+---------+----------+-----------------+-----+------+------------+--------+"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# read in the covid dataset\n","covid_df = spark.read.parquet('../data/curated/virals/covid/cleaned/cases-by-day')\n","covid_df.limit(5)\n","# TODO: commenting"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["COVID_GROUP_COLUMNS = [\n","    'week_year',\n","    'week_month',\n","    'week_ending',\n","    'week_index',\n","    'timeline',\n","    'borough'\n","]\n","# TODO: commenting\n","COVID_AGGREGATE_COLUMNS = {\n","    'cases': ['total', 'daily_average'],\n","    'deaths': ['total', 'daily_average'],\n","    'hospitalised': ['total', 'daily_average'],\n","}"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["covid_df = ah.group_and_aggregate(covid_df, COVID_GROUP_COLUMNS, \n","    COVID_AGGREGATE_COLUMNS)\n","\n","# force this into memory \n","# otherwise writing parquets results in a java executor out of memory error\n","covid_df = spark.createDataFrame(covid_df.collect())\n","# TODO: commenting"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["if INTERMEDIATE_OUTPUTS:\n","    covid_df.sort('week_index').limit(5)\n","# TODO: commenting"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# save it\n","# TODO: commenting\n","covid_df.write.mode('overwrite').parquet('../data/curated/virals/covid/aggregated/cases-by-week')"]},{"cell_type":"markdown","metadata":{},"source":["### 3. Aggregating the Flu dataset\n","*The flu dataset is already grouped by MMWR week, so only daily_averages can be calculated*"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"text/html":["<table border='1'>\n","<tr><th>year</th><th>month</th><th>day</th><th>week_ending</th><th>week_year</th><th>week_month</th><th>timeline</th><th>date</th><th>borough</th><th>disease</th><th>cases</th></tr>\n","<tr><td>2018</td><td>1</td><td>20</td><td>2018-01-20</td><td>2018</td><td>1</td><td>neither</td><td>01/20/2018</td><td>Bronx</td><td>INFLUENZA_B</td><td>203</td></tr>\n","<tr><td>2018</td><td>5</td><td>5</td><td>2018-05-05</td><td>2018</td><td>5</td><td>pre</td><td>05/05/2018</td><td>Staten Island</td><td>INFLUENZA_UNSPECI...</td><td>0</td></tr>\n","<tr><td>2018</td><td>3</td><td>10</td><td>2018-03-10</td><td>2018</td><td>3</td><td>pre</td><td>03/10/2018</td><td>Manhattan</td><td>INFLUENZA_A</td><td>65</td></tr>\n","<tr><td>2018</td><td>3</td><td>3</td><td>2018-03-03</td><td>2018</td><td>2</td><td>neither</td><td>03/03/2018</td><td>Brooklyn</td><td>INFLUENZA_B</td><td>287</td></tr>\n","<tr><td>2018</td><td>5</td><td>12</td><td>2018-05-12</td><td>2018</td><td>5</td><td>pre</td><td>05/12/2018</td><td>Manhattan</td><td>INFLUENZA_B</td><td>10</td></tr>\n","</table>\n"],"text/plain":["+----+-----+---+-----------+---------+----------+--------+----------+-------------+--------------------+-----+\n","|year|month|day|week_ending|week_year|week_month|timeline|      date|      borough|             disease|cases|\n","+----+-----+---+-----------+---------+----------+--------+----------+-------------+--------------------+-----+\n","|2018|    1| 20| 2018-01-20|     2018|         1| neither|01/20/2018|        Bronx|         INFLUENZA_B|  203|\n","|2018|    5|  5| 2018-05-05|     2018|         5|     pre|05/05/2018|Staten Island|INFLUENZA_UNSPECI...|    0|\n","|2018|    3| 10| 2018-03-10|     2018|         3|     pre|03/10/2018|    Manhattan|         INFLUENZA_A|   65|\n","|2018|    3|  3| 2018-03-03|     2018|         2| neither|03/03/2018|     Brooklyn|         INFLUENZA_B|  287|\n","|2018|    5| 12| 2018-05-12|     2018|         5|     pre|05/12/2018|    Manhattan|         INFLUENZA_B|   10|\n","+----+-----+---+-----------+---------+----------+--------+----------+-------------+--------------------+-----+"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["# read in the flu dataset\n","flu_df = spark.read.parquet('../data/curated/virals/flu/cleaned/cases-by-week')\n","flu_df.limit(5)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["FLU_GROUP_COLUMNS = [\n","    'week_year',\n","    'week_month',\n","    'week_ending',\n","    'week_index',\n","    'timeline',\n","    'borough'\n","]\n","# TODO: commenting\n","FLU_AGGREGATE_COLUMNS = {\n","    'cases': ['total', 'daily_average'],\n","}"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"ename":"AnalysisException","evalue":"Column 'week_index' does not exist. Did you mean one of the following? [week_ending, week_month, week_year, timeline, cases, date, day, disease, month, year, borough];\n'Aggregate [week_year#762L, week_month#763L, week_ending#761, 'week_index, timeline#764, borough#766], [week_year#762L, week_month#763L, week_ending#761, 'week_index, timeline#764, borough#766, sum(cast(cases#768 as double)) AS tot_cases#883, (sum(cast(cases#768 as double)) / cast(7 as double)) AS daily_avg_cases#885]\n+- Relation [year#758,month#759,day#760,week_ending#761,week_year#762L,week_month#763L,timeline#764,date#765,borough#766,disease#767,cases#768] parquet\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/mnt/c/Users/Xavier Travers/Desktop/Uni/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_2_aggregation.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Xavier%20Travers/Desktop/Uni/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_2_aggregation.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m flu_df \u001b[39m=\u001b[39m ah\u001b[39m.\u001b[39;49mgroup_and_aggregate(flu_df, FLU_GROUP_COLUMNS, \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Xavier%20Travers/Desktop/Uni/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_2_aggregation.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     FLU_AGGREGATE_COLUMNS)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Xavier%20Travers/Desktop/Uni/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_2_aggregation.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# force this into memory \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Xavier%20Travers/Desktop/Uni/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_2_aggregation.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# otherwise writing parquets results in a java executor out of memory error\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Xavier%20Travers/Desktop/Uni/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_2_aggregation.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m flu_df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame(flu_df\u001b[39m.\u001b[39mcollect())\n","File \u001b[0;32m/mnt/c/Users/Xavier Travers/Desktop/Uni/ADS/mast30034-project-1-DigitalData/notebooks/../scripts/helpers/aggregation_helpers.py:28\u001b[0m, in \u001b[0;36mgroup_and_aggregate\u001b[0;34m(df, group_cols, agg_cols)\u001b[0m\n\u001b[1;32m     23\u001b[0m         prefix, func \u001b[39m=\u001b[39m AGGREGATION_FUNCTIONS[func_type]\n\u001b[1;32m     24\u001b[0m         column_aggregates\u001b[39m.\u001b[39mappend(\n\u001b[1;32m     25\u001b[0m             func(colname)\u001b[39m.\u001b[39malias(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mprefix\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mcolname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m         )\n\u001b[0;32m---> 28\u001b[0m \u001b[39mreturn\u001b[39;00m grouped_df\u001b[39m.\u001b[39;49magg(\u001b[39m*\u001b[39;49mcolumn_aggregates)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/group.py:137\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(c, Column) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m exprs), \u001b[39m\"\u001b[39m\u001b[39mall exprs should be Column\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m     exprs \u001b[39m=\u001b[39m cast(Tuple[Column, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m], exprs)\n\u001b[0;32m--> 137\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jgd\u001b[39m.\u001b[39;49magg(exprs[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49m_jc, _to_seq(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49m_sc, [c\u001b[39m.\u001b[39;49m_jc \u001b[39mfor\u001b[39;49;00m c \u001b[39min\u001b[39;49;00m exprs[\u001b[39m1\u001b[39;49m:]]))\n\u001b[1;32m    138\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrame(jdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n","\u001b[0;31mAnalysisException\u001b[0m: Column 'week_index' does not exist. Did you mean one of the following? [week_ending, week_month, week_year, timeline, cases, date, day, disease, month, year, borough];\n'Aggregate [week_year#762L, week_month#763L, week_ending#761, 'week_index, timeline#764, borough#766], [week_year#762L, week_month#763L, week_ending#761, 'week_index, timeline#764, borough#766, sum(cast(cases#768 as double)) AS tot_cases#883, (sum(cast(cases#768 as double)) / cast(7 as double)) AS daily_avg_cases#885]\n+- Relation [year#758,month#759,day#760,week_ending#761,week_year#762L,week_month#763L,timeline#764,date#765,borough#766,disease#767,cases#768] parquet\n"]}],"source":["flu_df = ah.group_and_aggregate(flu_df, FLU_GROUP_COLUMNS, \n","    FLU_AGGREGATE_COLUMNS)\n","\n","# force this into memory \n","# otherwise writing parquets results in a java executor out of memory error\n","flu_df = spark.createDataFrame(flu_df.collect())\n","# TODO: commenting"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"ename":"AnalysisException","evalue":"Column 'week_index' does not exist. Did you mean one of the following? [week_ending, week_month, week_year, timeline, tot_cases, borough, daily_avg_cases];\n'Sort ['week_index ASC NULLS FIRST], true\n+- LogicalRDD [week_year#744L, week_month#745L, week_ending#746, timeline#747, borough#748, tot_cases#749, daily_avg_cases#750], false\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/mnt/c/Users/Xavier Travers/Desktop/Uni/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_2_aggregation.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Xavier%20Travers/Desktop/Uni/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_2_aggregation.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# if INTERMEDIATE_OUTPUTS:\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Xavier%20Travers/Desktop/Uni/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_2_aggregation.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m flu_df\u001b[39m.\u001b[39;49msort(\u001b[39m'\u001b[39;49m\u001b[39mweek_index\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mlimit(\u001b[39m5\u001b[39m)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py:1730\u001b[0m, in \u001b[0;36mDataFrame.sort\u001b[0;34m(self, *cols, **kwargs)\u001b[0m\n\u001b[1;32m   1695\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msort\u001b[39m(\n\u001b[1;32m   1696\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcols: Union[\u001b[39mstr\u001b[39m, Column, List[Union[\u001b[39mstr\u001b[39m, Column]]], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[1;32m   1697\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDataFrame\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1698\u001b[0m     \u001b[39m\"\"\"Returns a new :class:`DataFrame` sorted by the specified column(s).\u001b[39;00m\n\u001b[1;32m   1699\u001b[0m \n\u001b[1;32m   1700\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[39m    [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\u001b[39;00m\n\u001b[1;32m   1729\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1730\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49msort(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sort_cols(cols, kwargs))\n\u001b[1;32m   1731\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(jdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparkSession)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n","\u001b[0;31mAnalysisException\u001b[0m: Column 'week_index' does not exist. Did you mean one of the following? [week_ending, week_month, week_year, timeline, tot_cases, borough, daily_avg_cases];\n'Sort ['week_index ASC NULLS FIRST], true\n+- LogicalRDD [week_year#744L, week_month#745L, week_ending#746, timeline#747, borough#748, tot_cases#749, daily_avg_cases#750], false\n"]}],"source":["if INTERMEDIATE_OUTPUTS:\n","    flu_df.sort('week_index').limit(5)\n","# TODO: commenting"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["flu_df.write.mode('overwrite').parquet('../data/curated/virals/flu/aggregated/cases-by-week')"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":2}
