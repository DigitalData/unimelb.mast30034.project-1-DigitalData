{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAST30034: Applied Data Science Project 1\n",
    "---\n",
    "# Preprocessing Part 1: Cleaning The Data\n",
    "#### Xavier Travers (1178369)\n",
    "\n",
    "Cleaning the datasets of null, inconsistent, or unnecessary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports used throughout this notebook\n",
    "from collections import defaultdict\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# add homemade helpers\n",
    "sys.path.insert(1, '../../scripts')\n",
    "import helpers.cleaning_helpers as ch\n",
    "import helpers.join_helpers as jh\n",
    "\n",
    "# path where the data files are stored\n",
    "DATA_PATH = '../../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName('MAST30034 XT Project 1')\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True) \n",
    "    .config('spark.sql.parquet.cacheMetadata', 'true')\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the cdc week file to convert all dates to cdc weeks now\n",
    "mmwr_weeks_df = spark.read.parquet(f'{DATA_PATH}/raw/virals/mmwr_weeks.parquet')\n",
    "mmwr_weeks_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count rows\n",
    "mmwr_weeks_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the zones dataset\n",
    "zones_df = spark.read.csv(f'{DATA_PATH}/raw/tlc_zones/zones.csv',\n",
    "    header = True)\n",
    "zones_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count rows\n",
    "zones_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Join population statistics with zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the populations data\n",
    "pop_df_2010_2019 = pd.read_excel(f'{DATA_PATH}/raw/populations/2010_2019.xlsx',\n",
    "    header = [3], index_col=0).transpose()\n",
    "\n",
    "pop_df_2010_2019.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count rows\n",
    "pop_df_2010_2019.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the new york city boroughs\n",
    "pop_df_2010_2019 = pop_df_2010_2019[[\n",
    "    '.Bronx County, New York',\n",
    "    '.Kings County, New York',\n",
    "    '.New York County, New York',\n",
    "    '.Queens County, New York',\n",
    "    '.Richmond County, New York']]\n",
    "\n",
    "# rename the columns to their corresponding boroughs\n",
    "pop_df_2010_2019.columns = [\n",
    "    'Bronx',\n",
    "    'Brooklyn',\n",
    "    'Manhattan',\n",
    "    'Queens',\n",
    "    'Staten Island'\n",
    "]\n",
    "pop_df_2010_2019.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the populations data\n",
    "pop_df_2020_2021 = pd.read_excel(f'{DATA_PATH}/raw/populations/2020_2021.xlsx',\n",
    "    header = [3], index_col=0).transpose()\n",
    "\n",
    "pop_df_2020_2021.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count rows\n",
    "pop_df_2020_2021.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the new york city boroughs\n",
    "pop_df_2020_2021 = pop_df_2020_2021[[\n",
    "    '.Bronx County, New York',\n",
    "    '.Kings County, New York',\n",
    "    '.New York County, New York',\n",
    "    '.Queens County, New York',\n",
    "    '.Richmond County, New York']]\n",
    "\n",
    "# rename the columns to their corresponding boroughs\n",
    "pop_df_2020_2021.columns = [\n",
    "    'Bronx',\n",
    "    'Brooklyn',\n",
    "    'Manhattan',\n",
    "    'Queens',\n",
    "    'Staten Island'\n",
    "]\n",
    "pop_df_2020_2021.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the population data\n",
    "pop_df = pd.concat(\n",
    "    [pop_df_2010_2019, pop_df_2020_2021]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep 2018-2021 data\n",
    "pop_df = pop_df.filter(items = [2018, 2019, 2020, 2021], axis = 0)\n",
    "pop_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features\n",
    "len(pop_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the index column\n",
    "pop_df.index.name = 'week_year'\n",
    "pop_df = pop_df.reset_index()\n",
    "pop_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verticalize the data (I just want a year column and a borough column)\n",
    "temp_df = None\n",
    "for col in pop_df.columns[1:]:\n",
    "    # add the borough to an extracted borough column\n",
    "    # using .copy() hides a space-hog warning about not editing pop_df \n",
    "    # (which is my intention)\n",
    "    b_pop_df = pop_df[['week_year', col]].copy()\n",
    "    b_pop_df.columns = ['week_year', 'population']\n",
    "    b_pop_df['borough'] = col\n",
    "\n",
    "    if temp_df is None:\n",
    "        temp_df = b_pop_df \n",
    "    else:\n",
    "        temp_df = pd.concat([temp_df, b_pop_df])\n",
    "\n",
    "# set the verticalized data to the population data\n",
    "pop_df = spark.createDataFrame(temp_df)\n",
    "pop_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the population df\n",
    "pop_df.write.mode('overwrite').parquet(f'{DATA_PATH}/curated/population_by_borough_by_year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleaning the TLC dataset(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in an example to see the datatypes\n",
    "example_df = spark.read.parquet(f'{DATA_PATH}/raw/tlc/yellow/2019_12.parquet/')\n",
    "example_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are large trip distances that I need to filter out\n",
    "example_df.sort('trip_distance', ascending = False).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the tlc data\n",
    "tlc_df = jh.read_stacked_tlc_df(spark)\n",
    "tlc_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the width of the tlc df\n",
    "len(tlc_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the amount of raw rows\n",
    "tlc_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive extra values which are used to filter out valid trips\n",
    "SECONDS_TO_HOURS = 1 / (60*60)\n",
    "tlc_df = tlc_df\\\n",
    "    .withColumn('hours_elapsed', \n",
    "        ( # counts how long the trip was in hours\n",
    "            (F.col(\"tpep_dropoff_datetime\").cast(\"long\")\n",
    "            - F.col('tpep_pickup_datetime').cast(\"long\")) \n",
    "            * SECONDS_TO_HOURS\n",
    "        )\n",
    "    )\\\n",
    "    .withColumn('mph', \n",
    "        ( # calculate the speed of the trip\n",
    "            F.col('trip_distance') / F.col('hours_elapsed')\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the columns look correct\n",
    "tlc_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ypdcrime.com/vt/article30.php?zoom_highlight=fifty+five+miles+per+hour#t1180-a\n",
    "# As per: https://www.dot.ny.gov/divisions/operating/oom/transportation-systems/repository/TSMI-17-05.pdf\n",
    "# the NYS maximum speed limit is 65 mph. filter out trips faster than legal.\n",
    "tlc_df = tlc_df.where(\n",
    "    (F.col('mph').isNotNull()) &\n",
    "    (F.col('mph') <= 65)\n",
    ")\n",
    "\n",
    "# this legitimately removes any invalid trips \n",
    "# (and other null values are culled later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: this one is time intensive \n",
    "# check for large distance trips \n",
    "# (as long as the timing seems reasonable, they are kept)\n",
    "tlc_df.sort('trip_distance', ascending = False).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: this one is time intensive \n",
    "# next, filter out trips which do not start  within the 5 boroughs \n",
    "tlc_df = ch.extract_borough_name(\n",
    "    tlc_df.withColumnRenamed('PULocationID', 'pu_location_id'), zones_df,  'pu')\n",
    "# tlc_df = ch.extract_borough_name(tlc_df, zones_df,  'do')\n",
    "tlc_df.sort('trip_distance', ascending=False).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of the tlc datasets to clean \n",
    "# (I was originally planning on working on fhvhv and green as well)\n",
    "TLC_NAMES = ['yellow']\n",
    "\n",
    "# dictionary to rename all the columns I want to keep\n",
    "TLC_KEEP_COLUMNS = {\n",
    "    'tpep_pickup_datetime': 'date',\n",
    "    # 'passenger_count': 'passengers',\n",
    "    'trip_distance': 'trip_distance',\n",
    "    'pu_borough': 'pu_borough',\n",
    "    # 'DOLocationID': 'do_location_id',\n",
    "    'hours_elapsed': 'hours_elapsed'\n",
    "    # #  below only apply to fhvhv\n",
    "    # 'hvfhs_license_num': 'fhvhv_license',\n",
    "    # 'pickup_datetime': 'date',\n",
    "    # 'trip_miles': 'trip_distance',\n",
    "    # 'shared_request_flag': 'shared'\n",
    "}\n",
    "\n",
    "# create a dictionary of the columns to keep and the required filters\n",
    "TLC_CLEAN_COLUMNS = {\n",
    "    'pu_location_id': [ch.non_null], \n",
    "    # 'do_location_id': [ch.non_null], \n",
    "    # 'passengers': [ch.non_null], \n",
    "    'trip_distance': [ch.non_null, ch.non_negative], \n",
    "    # #  below only apply to fhvhv\n",
    "    # 'fhvhv_license': [ch.non_null], \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the drawn out cleaning process (function in `scripts/helpers`)\n",
    "tlc_df = ch.perform_cleaning(tlc_df, mmwr_weeks_df, TLC_KEEP_COLUMNS, \n",
    "    TLC_CLEAN_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: this one is time intensive \n",
    "# make sure that the latest data pads the used timeline \n",
    "# (so I'm not missing any weeks in the final timeline)\n",
    "tlc_df.sort('week_index', ascending = False).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: this one is time intensive \n",
    "# double check for any trip distance outliers\n",
    "tlc_df.sort('trip_distance', ascending=False).limit(5)\n",
    "\n",
    "# my cleaning seems to have worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: this one is time intensive \n",
    "# count the number of rows after cleaning\n",
    "tlc_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: this one is time intensive \n",
    "# save the stacked df by month (this will take a while)\n",
    "# sorting first prevents the partitioning part of the write from crashing\n",
    "# (form personal experience)\n",
    "tlc_df = tlc_df.sort('week_year', 'week_month')\n",
    "tlc_df.write\\\n",
    "    .partitionBy('week_year', 'week_month')\\\n",
    "    .mode('overwrite')\\\n",
    "    .parquet(f'{DATA_PATH}/curated/tlc/cleaned/yellow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cleaning the COVID dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the covid dataset\n",
    "covid_df = spark.read.csv(f'{DATA_PATH}/raw/virals/covid/cases_by_day.csv',\n",
    "    header = True, inferSchema=True)\n",
    "covid_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the width of the df\n",
    "len(covid_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the amount of raw rows\n",
    "covid_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum the number of incomplete datasets (ensure no incomplete values)\n",
    "sum(covid_df.select('INCOMPLETE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to rename all the columns I want to keep\n",
    "COVID_KEEP_COLUMNS = {\n",
    "    'date_of_interest':'date'\n",
    "}\n",
    "\n",
    "# create a dictionary of the columns to keep and the required filters\n",
    "COVID_CLEAN_COLUMNS = defaultdict(lambda: ch.non_negative)\n",
    "\n",
    "# define the boroughs as they appear in columns of the covid dataset\n",
    "COVID_BOROUGHS = {\n",
    "    'BX_':'Bronx',\n",
    "    'BK_':'Brooklyn',\n",
    "    'MN_':'Manhattan',\n",
    "    'QN_':'Queens',\n",
    "    'SI_':'Staten Island',\n",
    "}\n",
    "\n",
    "# define the count values to extract\n",
    "COVID_COUNTS = {\n",
    "    'CASE_COUNT': 'cases', \n",
    "    # 'DEATH_COUNT': 'deaths', \n",
    "    # 'HOSPITALIZED_COUNT': 'hospitalised'\n",
    "}\n",
    "\n",
    "# got throuch each borough and count value and add them to the columns to keep\n",
    "for prefix, new_prefix in COVID_BOROUGHS.items():\n",
    "    for suffix, new_suffix in COVID_COUNTS.items():\n",
    "        COVID_KEEP_COLUMNS[f'{prefix}{suffix}'] = f'{new_prefix}{new_suffix}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the drawn out cleaning process (function in `scripts/helpers`)\n",
    "covid_df = ch.perform_cleaning(covid_df, mmwr_weeks_df, COVID_KEEP_COLUMNS, \n",
    "    COVID_CLEAN_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date columns to keep after verticalising the covid data\n",
    "COVID_DATE_COLUMNS = [\n",
    "    F.col('date'), \n",
    "    F.col('week_ending'), \n",
    "    F.col('week_year'), \n",
    "    F.col('week_month'), \n",
    "    F.col('week_index')\n",
    "]\n",
    "\n",
    "# verticalise this dataset\n",
    "# I'd rather just have a 'borough' column for homogeneity of all the data\n",
    "temp_df = None\n",
    "for prefix in COVID_BOROUGHS.values():\n",
    "    # derive the columns for this borough to extract and stack\n",
    "    borough_columns = []\n",
    "    for suffix in COVID_COUNTS.values():\n",
    "        borough_columns.append(F.col(f'{prefix}{suffix}').alias(suffix))\n",
    "\n",
    "    # extract the counts for this borough and add them to the stacked dataframe\n",
    "    if temp_df == None:\n",
    "        temp_df = covid_df.select(COVID_DATE_COLUMNS + borough_columns)\\\n",
    "            .withColumn('borough', F.lit(prefix))\n",
    "    else:\n",
    "        temp_df = temp_df\\\n",
    "            .union(\n",
    "                covid_df.select(COVID_DATE_COLUMNS + borough_columns)\\\n",
    "                    .withColumn('borough', F.lit(prefix))\n",
    "            )\n",
    "\n",
    "# set the df to the stacked data\n",
    "covid_df = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the null cases (created from the one-sided outer join for the df) with 0\n",
    "covid_df = covid_df.fillna(0, 'cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check that dates of weeks are correctly added/derived\n",
    "covid_df.sort('week_index', 'date').limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned covid data\n",
    "covid_df.write.mode('overwrite').parquet(f'{DATA_PATH}/curated/virals/covid/cleaned/cases_by_day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cleaning the flu dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the flu dataset\n",
    "flu_df = spark.read.csv(f'{DATA_PATH}/raw/virals/flu/cases_by_week.csv',\n",
    "    header=True, inferSchema=True)\n",
    "flu_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the width of the df\n",
    "len(flu_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the amount of raw rows\n",
    "flu_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of distinct counties (column now called 'borough')\n",
    "flu_df.select('County').distinct().limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the boroughs to their proper names\n",
    "# from: https://portal.311.nyc.gov/article/?kanumber=KA-02877\n",
    "# also from map dict\n",
    "FLU_COUNTY_TO_BOROUGH = {\n",
    "    'BRONX': 'Bronx',\n",
    "    'KINGS': 'Brooklyn',\n",
    "    'NEW YORK': 'Manhattan',\n",
    "    'QUEENS': 'Queens',\n",
    "    'RICHMOND': 'Staten Island'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the mapping to the flu df\n",
    "flu_df = ch.replace_column_using_dict(flu_df, 'County', FLU_COUNTY_TO_BOROUGH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to rename all the columns I want to keep\n",
    "FLU_KEEP_COLUMNS = {\n",
    "    'Week Ending Date': 'date',\n",
    "    'County': 'borough',\n",
    "    'Disease': 'disease',\n",
    "    'Count': 'cases',\n",
    "}\n",
    "\n",
    "# create a dictionary of the columns to keep and the required filters\n",
    "FLU_CLEAN_COLUMNS = {\n",
    "    'date': [],\n",
    "    'region': [lambda _: F.col('region') == 'NYC'],\n",
    "    'borough': [],\n",
    "    'disease': [],\n",
    "    'cases': [ch.non_negative]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the drawn out cleaning process (function in `scripts/helpers`)\n",
    "flu_df = ch.perform_cleaning(flu_df, mmwr_weeks_df, FLU_KEEP_COLUMNS, \n",
    "    FLU_CLEAN_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the null cases (created from the one-sided outer join for the df) with 0\n",
    "flu_df = flu_df.fillna(0, 'cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that converting the boroughs worked\n",
    "flu_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned flu data\n",
    "flu_df.write.mode('overwrite').parquet(f'{DATA_PATH}/curated/virals/flu/cleaned/cases_by_week')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
