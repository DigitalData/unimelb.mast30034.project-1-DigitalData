{"cells":[{"cell_type":"markdown","metadata":{},"source":["### MAST30034: Applied Data Science Project 1\n","---\n","# Preprocessing Part 2: Aggregating Data by MMWR Week\n","#### Xavier Travers (1178369)\n","\n","Aggregate all the data by MMWR week (defined [here](https://ndc.services.cdc.gov/wp-content/uploads/MMWR_Week_overview.pdf))\n","and borough.\n","This means counting trips to and from each of the boroughs per month.\n","This is done for each of the taxi types."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# imports used throughout this notebook\n","from pyspark.sql import functions as F\n","import sys\n","\n","# add homemade helpers\n","sys.path.insert(1, '../../scripts')\n","import helpers.aggregation_helpers as ah\n","\n","# path where the data files are stored\n","DATA_PATH = '../../data'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession\n","\n","# Create a spark session (which will run spark jobs)\n","spark = (\n","    SparkSession.builder.appName('MAST30034 XT Project 1')\n","    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n","    .config('spark.sql.repl.eagerEval.enabled', True) \n","    .config('spark.sql.parquet.cacheMetadata', 'true')\n","    .config(\"spark.executor.memory\", \"2g\")\n","    .config(\"spark.driver.memory\", \"4g\")\n","    .getOrCreate()\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# read in the population data\n","pop_df = spark.read.parquet(f'{DATA_PATH}/curated/population_by_borough_by_year')\n","pop_df.limit(5)"]},{"cell_type":"markdown","metadata":{},"source":["### 1. Aggregating the TLC dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# read in the cleaned yellow dataset\n","tlc_df = spark.read.parquet(f'{DATA_PATH}/curated/tlc/cleaned/yellow')\n","\n","# count the raw # of rows and print it out just to check the formatting\n","f'{tlc_df.count()} ROWS'\n","tlc_df.limit(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# columns to group by \n","TLC_COMMON_GROUP_COLUMNS = [\n","    'week_year',\n","    'week_month',\n","    'week_ending',\n","    'week_index',\n","]\n","\n","# add the borough type to groupby\n","TLC_GROUP_BY_PU_COLUMNS = TLC_COMMON_GROUP_COLUMNS + ['pu_borough'];\n","TLC_GROUP_BY_DO_COLUMNS = TLC_COMMON_GROUP_COLUMNS + ['do_borough'];\n","\n","# determine how the values are to be aggregated\n","TLC_AGGREGATE_COLUMNS = {\n","    '*': [\n","        'count', \n","        'count_per_capita',\n","        'count_per_100k'\n","    ],\n","    # 'passengers': [\n","    #     'total', \n","    #     'total_per_capita', \n","    #     'average'\n","    # ],\n","    'trip_distance': [\n","        # 'total', \n","        # 'total_per_capita', \n","        'average'\n","    ],\n","    # 'hours_elapsed': [\n","    #     'total', \n","    #     'total_per_capita', \n","    #     'average'\n","    # ],\n","}"]},{"cell_type":"markdown","metadata":{},"source":["#### Group by pick-up location"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# perform the grouping and aggregation in (function in `scripts/helpers`)\n","tlc_by_pu_df = ah.group_and_aggregate(tlc_df, pop_df, TLC_GROUP_BY_PU_COLUMNS, \n","    TLC_AGGREGATE_COLUMNS)\n","\n","# force this into memory \n","# otherwise writing parquets results in a java executor out of memory error\n","# (this is personal experience, your mileage may vary)\n","tlc_by_pu_df = spark.createDataFrame(tlc_by_pu_df.collect())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# check to see that aggregation was successful\n","tlc_by_pu_df.limit(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# check to see that aggregation was successful\n","tlc_by_pu_df.sort('week_ending', ascending = False).limit(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# check the average distances for potential outliers? (unlikely after cleaning)\n","tlc_by_pu_df.sort('avg_trip_distance', ascending=False).limit(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save the aggregated by pickup data\n","tlc_by_pu_df.write.mode('overwrite')\\\n","    .parquet(f'{DATA_PATH}/curated/tlc/aggregated/yellow/by_pu')"]},{"cell_type":"markdown","metadata":{},"source":["### 2. Aggregating the COVID dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# read in the covid dataset\n","covid_df = spark.read.parquet(f'{DATA_PATH}/curated/virals/covid/cleaned/cases_by_day')\n","covid_df.limit(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# check that this all worked correctly\n","covid_df.sort('week_index', ascending = False).limit(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# columns to group by \n","COVID_GROUP_COLUMNS = [\n","    'week_year',\n","    'week_month',\n","    'week_ending',\n","    'week_index',\n","    'borough'\n","]\n","\n","# determine how the values are to be aggregated\n","COVID_AGGREGATE_COLUMNS = {\n","    'cases': [\n","        'total', \n","        'total_per_capita', \n","        'total_per_100k',\n","        # 'daily_average', \n","        # 'daily_average_per_capita'\n","    ],\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# perform the grouping and aggregation in (function in `scripts/helpers`)\n","covid_df = ah.group_and_aggregate(covid_df, pop_df, COVID_GROUP_COLUMNS, \n","    COVID_AGGREGATE_COLUMNS)\n","\n","# force this into memory \n","# otherwise writing parquets results in a java executor out of memory error\n","# (this is personal experience, your mileage may vary)\n","covid_df = spark.createDataFrame(covid_df.collect())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# check to see that aggregation was successful \n","covid_df.limit(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# check to see that aggregation was successful \n","covid_df.sort('week_ending', ascending = False).limit(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save the aggregated data\n","covid_df.write.mode('overwrite')\\\n","    .parquet(f'{DATA_PATH}/curated/virals/covid/aggregated/cases_by_week')"]},{"cell_type":"markdown","metadata":{},"source":["### 3. Aggregating the Flu dataset\n","*The flu dataset is already grouped by MMWR week, so only daily_averages can be calculated*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# read in the flu dataset\n","flu_df = spark.read.parquet(f'{DATA_PATH}/curated/virals/flu/cleaned/cases_by_week')\n","flu_df.limit(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# columns to group by \n","FLU_GROUP_COLUMNS = [\n","    'week_year',\n","    'week_month',\n","    'week_ending',\n","    'week_index',\n","    'borough'\n","]\n","\n","# determine how the values are to be aggregated\n","FLU_AGGREGATE_COLUMNS = {\n","    'cases': [\n","        'total', \n","        'total_per_capita', \n","        'total_per_100k',\n","    ],\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# perform the grouping and aggregation in (function in `scripts/helpers`)\n","flu_df = ah.group_and_aggregate(flu_df, pop_df, FLU_GROUP_COLUMNS, \n","    FLU_AGGREGATE_COLUMNS)\n","\n","# force this into memory \n","# otherwise writing parquets results in a java executor out of memory error\n","# (this is personal experience, your mileage may vary)\n","flu_df = spark.createDataFrame(flu_df.collect())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# check to see that aggregation was successful \n","flu_df.limit(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# check to see that aggregation was successful \n","flu_df.sort('week_ending', ascending = False).limit(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save the aggregated data\n","flu_df.write.mode('overwrite')\\\n","    .parquet(f'{DATA_PATH}/curated/virals/flu/aggregated/cases_by_week')"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":2}
