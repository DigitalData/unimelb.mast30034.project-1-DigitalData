{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAST30034: Applied Data Science Project 1\n",
    "---\n",
    "# Data Analysis: Geospatial Visualization of Average Distance Travelled During Weeks with Maximum Viral Rates\n",
    "#### Xavier Travers (1178369)\n",
    "\n",
    "TODO: Geospatial Visualization of Average Distance Travelled During Weeks with Maximum Viral "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports used throughout this notebook\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import DataFrame, Column\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "\n",
    "# add homemade helpers\n",
    "sys.path.insert(1, '../scripts')\n",
    "import helpers.join_helpers as jh\n",
    "import helpers.plot_helpers as ph\n",
    "\n",
    "# path where the data files are stored\n",
    "DATA_PATH = '../../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName('MAST30034 XT Project 1')\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True) \n",
    "    .config('spark.sql.parquet.cacheMetadata', 'true')\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_gj = gpd.read_file(f'{DATA_PATH}/raw/tlc_zones/boroughs.geojson')\n",
    "borough_gj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the borough centroids\n",
    "borough_gj['centroid'] = borough_gj['geometry'].apply(lambda gs: (gs.centroid.y, gs.centroid.x))\n",
    "borough_gj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the aggregated covid dataset\n",
    "covid_df = spark.read.parquet(f'{DATA_PATH}/curated/virals/covid/aggregated/cases-by-week')\n",
    "covid_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flu_df = spark.read.parquet(f'{DATA_PATH}/curated/virals/flu/aggregated/cases-by-week')\n",
    "flu_df.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Pickup Borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the aggregated tlc by pickup dataset\n",
    "tlc_pu_df = spark.read.parquet(f'{DATA_PATH}/curated/tlc/aggregated/yellow/by_pu')\n",
    "tlc_pu_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the tlc, and covid data by week preceding \n",
    "# (i.e. the covid and flu cases of week one are join to the taxi data from \n",
    "# week two). \n",
    "# shows whether there is an immediate weekly correlation due to the viruses.\n",
    "joined_pu_df = jh.join_by_week_by_borough(tlc_pu_df, covid_df, 'covid')\n",
    "joined_pu_df = jh.join_by_week_by_borough(joined_pu_df, flu_df, 'flu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply timeline 2 to the joined pu df\n",
    "joined_pu_df = joined_pu_df\\\n",
    "    .where(F.col('timeline') == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph.geospatial_distances_when_max(joined_pu_df.toPandas(),\n",
    "    borough_gj, 'covid_tot_p100k_cases', 'covid', \n",
    "    'Maximum COVID-19 Cases Per 100k People (by MMWR Week)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph.geospatial_distances_when_max(joined_pu_df.toPandas(),\n",
    "    borough_gj, 'flu_tot_p100k_cases', 'flu', \n",
    "    'Maximum Influenza Cases Per 100k People (by MMWR Week)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map based on just the average trip radius over the whole time period\n",
    "ph.geospatial_average_distance(joined_pu_df.toPandas(), borough_gj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Dropoff Borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the aggregated tlc by pickup dataset\n",
    "tlc_do_df = spark.read.parquet(f'{DATA_PATH}/curated/tlc/aggregated/yellow/by_do')\n",
    "tlc_do_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the tlc, and covid data by week preceding \n",
    "# (i.e. the covid and flu cases of week one are join to the taxi data from \n",
    "# week two). \n",
    "# shows whether there is an immediate weekly correlation due to the viruses.\n",
    "joined_do_df = jh.join_by_week_by_borough(tlc_do_df, covid_df, 'covid')\n",
    "joined_do_df = jh.join_by_week_by_borough(joined_do_df, flu_df, 'flu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply timeline 2 to the joined pu df\n",
    "joined_do_df = joined_do_df\\\n",
    "    .where(F.col('timeline') == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph.geospatial_distances_when_max(joined_do_df.toPandas(),\n",
    "    borough_gj, 'covid_tot_p100k_cases', 'covid', \n",
    "    'Maximum COVID-19 Cases Per 100k People (by MMWR Week)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph.geospatial_distances_when_max(joined_do_df.toPandas(),\n",
    "    borough_gj, 'flu_tot_p100k_cases', 'flu', \n",
    "    'Maximum Influenza Cases Per 100k People (by MMWR Week)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map based on just the average trip radius over the whole time period\n",
    "ph.geospatial_average_distance(joined_do_df.toPandas(), borough_gj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
