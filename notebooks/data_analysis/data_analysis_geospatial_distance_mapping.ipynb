{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAST30034: Applied Data Science Project 1\n",
    "---\n",
    "# Data Analysis: Geospatial Mapping of Average Trip Radiuses/Distances\n",
    "#### Xavier Travers (1178369)\n",
    "\n",
    "Self-explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports used throughout this notebook\n",
    "import sys\n",
    "import geopandas as gpd\n",
    "\n",
    "# add homemade helpers\n",
    "sys.path.insert(1, '../../scripts')\n",
    "import helpers.join_helpers as jh\n",
    "import helpers.plot_helpers as ph\n",
    "\n",
    "# path where the data files are stored\n",
    "DATA_PATH = '../../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName('MAST30034 XT Project 1')\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True) \n",
    "    .config('spark.sql.parquet.cacheMetadata', 'true')\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the geojson borough outlines\n",
    "borough_gj = gpd.read_file(f'{DATA_PATH}/raw/tlc_zones/boroughs.geojson')\n",
    "borough_gj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of rows/columns\n",
    "borough_gj.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the borough centroids\n",
    "borough_gj['centroid'] = borough_gj['geometry'].apply(lambda gs: (gs.centroid.y, gs.centroid.x))\n",
    "borough_gj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the aggregated covid dataset\n",
    "covid_df = spark.read.parquet(f'{DATA_PATH}/curated/virals/covid/aggregated/cases_by_week')\n",
    "covid_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the flu data\n",
    "flu_df = spark.read.parquet(f'{DATA_PATH}/curated/virals/flu/aggregated/cases_by_week')\n",
    "flu_df.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Pickup Borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the aggregated tlc by pickup dataset\n",
    "tlc_pu_df = spark.read.parquet(f'{DATA_PATH}/curated/tlc/aggregated/yellow/by_pu')\n",
    "tlc_pu_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the tlc, and covid data by week preceding \n",
    "# (i.e. the covid and flu cases of week one are join to the taxi data from \n",
    "# week two). \n",
    "# shows whether there is an immediate weekly correlation due to the viruses.\n",
    "joined_pu_df = jh.join_by_week_by_borough(tlc_pu_df, covid_df, 'covid')\n",
    "joined_pu_df = jh.join_by_week_by_borough(joined_pu_df, flu_df, 'flu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and save the max covid19 cases distances\n",
    "ph.geospatial_distances_when_max(joined_pu_df.toPandas(),\n",
    "    borough_gj, 'covid_tot_p100k_cases', 'covid', \n",
    "    'Maximum COVID-19 Cases Per 100k People (by MMWR Week)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and save the max flu cases distances\n",
    "ph.geospatial_distances_when_max(joined_pu_df.toPandas(),\n",
    "    borough_gj, 'flu_tot_p100k_cases', 'flu', \n",
    "    'Maximum Influenza Cases Per 100k People (by MMWR Week)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map based on just the average trip radius over the whole time period\n",
    "ph.geospatial_average_distance(joined_pu_df.toPandas(), borough_gj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
