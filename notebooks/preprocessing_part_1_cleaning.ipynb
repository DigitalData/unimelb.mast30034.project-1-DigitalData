{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAST30034: Applied Data Science Project 1\n",
    "---\n",
    "# Preprocessing Part 1: Cleaning The Data\n",
    "#### Xavier Travers (1178369)\n",
    "\n",
    "Cleaning the datasets of null, inconsistent, or unnecessary values.\n",
    "This is performed on the TLC data and COVID data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports used throughout this notebook\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import DataFrame, Column\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# add homemade helpers\n",
    "sys.path.insert(1, '../scripts')\n",
    "import helpers.cleaning_helpers as ch\n",
    "import helpers.join_helpers as jh\n",
    "\n",
    "# Used for saving time (if you don't want sanity-check printouts)\n",
    "INTERMEDIATE_OUTPUTS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/14 02:23:07 WARN Utils: Your hostname, Polaris resolves to a loopback address: 127.0.1.1; using 172.26.235.73 instead (on interface eth0)\n",
      "22/08/14 02:23:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/14 02:23:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/08/14 02:23:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/08/14 02:23:09 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/08/14 02:23:09 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/08/14 02:23:09 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName('MAST30034 XT Project 1')\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True) \n",
    "    .config('spark.sql.parquet.cacheMetadata', 'true')\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>year</th><th>month</th><th>day</th><th>cdc_week</th><th>week_index</th><th>us_format</th><th>week_ending</th><th>week_month</th><th>week_year</th><th>timeline</th></tr>\n",
       "<tr><td>2017</td><td>12</td><td>31</td><td>1</td><td>1</td><td>12/31/2017</td><td>2018-01-06</td><td>1</td><td>2018</td><td>neither</td></tr>\n",
       "<tr><td>2018</td><td>1</td><td>1</td><td>1</td><td>1</td><td>01/01/2018</td><td>2018-01-06</td><td>1</td><td>2018</td><td>neither</td></tr>\n",
       "<tr><td>2018</td><td>1</td><td>2</td><td>1</td><td>1</td><td>01/02/2018</td><td>2018-01-06</td><td>1</td><td>2018</td><td>neither</td></tr>\n",
       "<tr><td>2018</td><td>1</td><td>3</td><td>1</td><td>1</td><td>01/03/2018</td><td>2018-01-06</td><td>1</td><td>2018</td><td>neither</td></tr>\n",
       "<tr><td>2018</td><td>1</td><td>4</td><td>1</td><td>1</td><td>01/04/2018</td><td>2018-01-06</td><td>1</td><td>2018</td><td>neither</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+-----+---+--------+----------+----------+-----------+----------+---------+--------+\n",
       "|year|month|day|cdc_week|week_index| us_format|week_ending|week_month|week_year|timeline|\n",
       "+----+-----+---+--------+----------+----------+-----------+----------+---------+--------+\n",
       "|2017|   12| 31|       1|         1|12/31/2017| 2018-01-06|         1|     2018| neither|\n",
       "|2018|    1|  1|       1|         1|01/01/2018| 2018-01-06|         1|     2018| neither|\n",
       "|2018|    1|  2|       1|         1|01/02/2018| 2018-01-06|         1|     2018| neither|\n",
       "|2018|    1|  3|       1|         1|01/03/2018| 2018-01-06|         1|     2018| neither|\n",
       "|2018|    1|  4|       1|         1|01/04/2018| 2018-01-06|         1|     2018| neither|\n",
       "+----+-----+---+--------+----------+----------+-----------+----------+---------+--------+"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the cdc week file to convert all dates to cdc weeks now\n",
    "mmwr_weeks_df = spark.read.parquet('../data/raw/virals/mmwr_weeks.parquet')\n",
    "mmwr_weeks_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the zones dataset\n",
    "zones_df = spark.read.csv('../data/raw/tlc_zones/zones.csv',\n",
    "    header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cleaning the TLC dataset(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERMEDIATE_OUTPUTS:\n",
    "    example_df = spark.read.parquet('../data/raw/tlc/yellow/2019-07.parquet/')\n",
    "    example_df.limit(5)\n",
    "# TODO: commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERMEDIATE_OUTPUTS:\n",
    "    example_df.sort('trip_distance', ascending = False).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of the tlc datasets to clean \n",
    "# (I was originally planning on working on fhvhv and green as well)\n",
    "TLC_NAMES = ['yellow']\n",
    "\n",
    "# dictionary to rename all the columns I want to keep\n",
    "TLC_KEEP_COLUMNS = {\n",
    "    'tpep_pickup_datetime': 'date',\n",
    "    'passenger_count': 'passengers',\n",
    "    'trip_distance': 'trip_distance',\n",
    "    'PULocationID': 'pu_location_id',\n",
    "    'DOLocationID': 'do_location_id',\n",
    "    'hours_elapsed': 'hours_elapsed'\n",
    "    # #  below only apply to fhvhv\n",
    "    # 'hvfhs_license_num': 'fhvhv_license',\n",
    "    # 'pickup_datetime': 'date',\n",
    "    # 'trip_miles': 'trip_distance',\n",
    "    # 'shared_request_flag': 'shared'\n",
    "}\n",
    "\n",
    "# create a dictionary of the columns to keep and the required filters\n",
    "TLC_CLEAN_COLUMNS = {\n",
    "    'pu_location_id': [ch.non_null], \n",
    "    'do_location_id': [ch.non_null], \n",
    "    'passengers': [ch.non_null], \n",
    "    'trip_distance': [ch.non_null, ch.non_negative], \n",
    "    # 'fhvhv_license': [ch.non_null], \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the tlc data\n",
    "tlc_df = jh.read_stacked_tlc_df(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERMEDIATE_OUTPUTS:\n",
    "    # sanity check\n",
    "    tlc_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive extra values which are used to filter out valid trips\n",
    "SECONDS_TO_HOURS = 1 / (60*60)\n",
    "tlc_df = tlc_df\\\n",
    "    .withColumn('hours_elapsed', \n",
    "        (\n",
    "            (F.col(\"tpep_dropoff_datetime\").cast(\"long\")\n",
    "            - F.col('tpep_pickup_datetime').cast(\"long\")) \n",
    "            * SECONDS_TO_HOURS\n",
    "        )\n",
    "    )\\\n",
    "    .withColumn('mph', (F.col('trip_distance') / F.col('hours_elapsed')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERMEDIATE_OUTPUTS:\n",
    "    tlc_df.limit(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ypdcrime.com/vt/article30.php?zoom_highlight=fifty+five+miles+per+hour#t1180-a.\n",
    "# As per: https://www.dot.ny.gov/divisions/operating/oom/transportation-systems/repository/TSMI-17-05.pdf\n",
    "# the NYS maximum speed limit is 65 mph. filter out trips faster than legal.\n",
    "tlc_df = tlc_df.where(\n",
    "    (F.col('mph').isNotNull()) &\n",
    "    (F.col('mph') <= 65)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERMEDIATE_OUTPUTS:\n",
    "    # this one is time instensive \n",
    "    tlc_df.sort('trip_distance', ascending = False).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlc_df = ch.perform_cleaning(tlc_df, mmwr_weeks_df, TLC_KEEP_COLUMNS, \n",
    "    TLC_CLEAN_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, filter out trips which do not start and/or end within the 5 boroughs \n",
    "tlc_df = ch.extract_borough_name(tlc_df, zones_df,  'pu')\n",
    "tlc_df = ch.extract_borough_name(tlc_df, zones_df,  'do')\n",
    "\n",
    "if INTERMEDIATE_OUTPUTS:\n",
    "    tlc_df.sort('trip_distance', ascending=False).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERMEDIATE_OUTPUTS:\n",
    "    tlc_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERMEDIATE_OUTPUTS:\n",
    "    tlc_df.sort('week_index', ascending = False).limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERMEDIATE_OUTPUTS:\n",
    "    tlc_df.sort('trip_distance', ascending=False).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by anything outside of the time bounded by the timelines\n",
    "# (keep the transitionary time (2019) for time-series analysis)\n",
    "tlc_df = tlc_df.where(F.col('timeline') != 'neither')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save the stacked df by month (this will take a while)\n",
    "tlc_df = tlc_df.sort('week_year', 'week_month')\n",
    "tlc_df.write\\\n",
    "    .partitionBy('week_year', 'week_month')\\\n",
    "    .mode('overwrite')\\\n",
    "    .parquet(f'../data/curated/tlc/cleaned/yellow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleaning the COVID dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/14 02:32:35 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>date_of_interest</th><th>CASE_COUNT</th><th>PROBABLE_CASE_COUNT</th><th>HOSPITALIZED_COUNT</th><th>DEATH_COUNT</th><th>PROBABLE_DEATH_COUNT</th><th>CASE_COUNT_7DAY_AVG</th><th>ALL_CASE_COUNT_7DAY_AVG</th><th>HOSP_COUNT_7DAY_AVG</th><th>DEATH_COUNT_7DAY_AVG</th><th>ALL_DEATH_COUNT_7DAY_AVG</th><th>BX_CASE_COUNT</th><th>BX_PROBABLE_CASE_COUNT</th><th>BX_HOSPITALIZED_COUNT</th><th>BX_DEATH_COUNT</th><th>BX_PROBABLE_DEATH_COUNT</th><th>BX_CASE_COUNT_7DAY_AVG</th><th>BX_PROBABLE_CASE_COUNT_7DAY_AVG</th><th>BX_ALL_CASE_COUNT_7DAY_AVG</th><th>BX_HOSPITALIZED_COUNT_7DAY_AVG</th><th>BX_DEATH_COUNT_7DAY_AVG</th><th>BX_ALL_DEATH_COUNT_7DAY_AVG</th><th>BK_CASE_COUNT</th><th>BK_PROBABLE_CASE_COUNT</th><th>BK_HOSPITALIZED_COUNT</th><th>BK_DEATH_COUNT</th><th>BK_PROBABLE_DEATH_COUNT</th><th>BK_CASE_COUNT_7DAY_AVG</th><th>BK_PROBABLE_CASE_COUNT_7DAY_AVG</th><th>BK_ALL_CASE_COUNT_7DAY_AVG</th><th>BK_HOSPITALIZED_COUNT_7DAY_AVG</th><th>BK_DEATH_COUNT_7DAY_AVG</th><th>BK_ALL_DEATH_COUNT_7DAY_AVG</th><th>MN_CASE_COUNT</th><th>MN_PROBABLE_CASE_COUNT</th><th>MN_HOSPITALIZED_COUNT</th><th>MN_DEATH_COUNT</th><th>MN_PROBABLE_DEATH_COUNT</th><th>MN_CASE_COUNT_7DAY_AVG</th><th>MN_PROBABLE_CASE_COUNT_7DAY_AVG</th><th>MN_ALL_CASE_COUNT_7DAY_AVG</th><th>MN_HOSPITALIZED_COUNT_7DAY_AVG</th><th>MN_DEATH_COUNT_7DAY_AVG</th><th>MN_ALL_DEATH_COUNT_7DAY_AVG</th><th>QN_CASE_COUNT</th><th>QN_PROBABLE_CASE_COUNT</th><th>QN_HOSPITALIZED_COUNT</th><th>QN_DEATH_COUNT</th><th>QN_PROBABLE_DEATH_COUNT</th><th>QN_CASE_COUNT_7DAY_AVG</th><th>QN_PROBABLE_CASE_COUNT_7DAY_AVG</th><th>QN_ALL_CASE_COUNT_7DAY_AVG</th><th>QN_HOSPITALIZED_COUNT_7DAY_AVG</th><th>QN_DEATH_COUNT_7DAY_AVG</th><th>QN_ALL_DEATH_COUNT_7DAY_AVG</th><th>SI_CASE_COUNT</th><th>SI_PROBABLE_CASE_COUNT</th><th>SI_HOSPITALIZED_COUNT</th><th>SI_DEATH_COUNT</th><th>SI_PROBABLE_DEATH_COUNT</th><th>SI_CASE_COUNT_7DAY_AVG</th><th>SI_PROBABLE_CASE_COUNT_7DAY_AVG</th><th>SI_ALL_CASE_COUNT_7DAY_AVG</th><th>SI_HOSPITALIZED_COUNT_7DAY_AVG</th><th>SI_DEATH_COUNT_7DAY_AVG</th><th>SI_ALL_DEATH_COUNT_7DAY_AVG</th><th>INCOMPLETE</th></tr>\n",
       "<tr><td>02/29/2020</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>03/01/2020</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>03/02/2020</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>03/03/2020</td><td>1</td><td>0</td><td>7</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>3</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>03/04/2020</td><td>5</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------------+----------+-------------------+------------------+-----------+--------------------+-------------------+-----------------------+-------------------+--------------------+------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+----------+\n",
       "|date_of_interest|CASE_COUNT|PROBABLE_CASE_COUNT|HOSPITALIZED_COUNT|DEATH_COUNT|PROBABLE_DEATH_COUNT|CASE_COUNT_7DAY_AVG|ALL_CASE_COUNT_7DAY_AVG|HOSP_COUNT_7DAY_AVG|DEATH_COUNT_7DAY_AVG|ALL_DEATH_COUNT_7DAY_AVG|BX_CASE_COUNT|BX_PROBABLE_CASE_COUNT|BX_HOSPITALIZED_COUNT|BX_DEATH_COUNT|BX_PROBABLE_DEATH_COUNT|BX_CASE_COUNT_7DAY_AVG|BX_PROBABLE_CASE_COUNT_7DAY_AVG|BX_ALL_CASE_COUNT_7DAY_AVG|BX_HOSPITALIZED_COUNT_7DAY_AVG|BX_DEATH_COUNT_7DAY_AVG|BX_ALL_DEATH_COUNT_7DAY_AVG|BK_CASE_COUNT|BK_PROBABLE_CASE_COUNT|BK_HOSPITALIZED_COUNT|BK_DEATH_COUNT|BK_PROBABLE_DEATH_COUNT|BK_CASE_COUNT_7DAY_AVG|BK_PROBABLE_CASE_COUNT_7DAY_AVG|BK_ALL_CASE_COUNT_7DAY_AVG|BK_HOSPITALIZED_COUNT_7DAY_AVG|BK_DEATH_COUNT_7DAY_AVG|BK_ALL_DEATH_COUNT_7DAY_AVG|MN_CASE_COUNT|MN_PROBABLE_CASE_COUNT|MN_HOSPITALIZED_COUNT|MN_DEATH_COUNT|MN_PROBABLE_DEATH_COUNT|MN_CASE_COUNT_7DAY_AVG|MN_PROBABLE_CASE_COUNT_7DAY_AVG|MN_ALL_CASE_COUNT_7DAY_AVG|MN_HOSPITALIZED_COUNT_7DAY_AVG|MN_DEATH_COUNT_7DAY_AVG|MN_ALL_DEATH_COUNT_7DAY_AVG|QN_CASE_COUNT|QN_PROBABLE_CASE_COUNT|QN_HOSPITALIZED_COUNT|QN_DEATH_COUNT|QN_PROBABLE_DEATH_COUNT|QN_CASE_COUNT_7DAY_AVG|QN_PROBABLE_CASE_COUNT_7DAY_AVG|QN_ALL_CASE_COUNT_7DAY_AVG|QN_HOSPITALIZED_COUNT_7DAY_AVG|QN_DEATH_COUNT_7DAY_AVG|QN_ALL_DEATH_COUNT_7DAY_AVG|SI_CASE_COUNT|SI_PROBABLE_CASE_COUNT|SI_HOSPITALIZED_COUNT|SI_DEATH_COUNT|SI_PROBABLE_DEATH_COUNT|SI_CASE_COUNT_7DAY_AVG|SI_PROBABLE_CASE_COUNT_7DAY_AVG|SI_ALL_CASE_COUNT_7DAY_AVG|SI_HOSPITALIZED_COUNT_7DAY_AVG|SI_DEATH_COUNT_7DAY_AVG|SI_ALL_DEATH_COUNT_7DAY_AVG|INCOMPLETE|\n",
       "+----------------+----------+-------------------+------------------+-----------+--------------------+-------------------+-----------------------+-------------------+--------------------+------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+----------+\n",
       "|      02/29/2020|         1|                  0|                 1|          0|                   0|                  0|                      0|                  0|                   0|                       0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    1|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            1|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|         0|\n",
       "|      03/01/2020|         0|                  0|                 1|          0|                   0|                  0|                      0|                  0|                   0|                       0|            0|                     0|                    1|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|         0|\n",
       "|      03/02/2020|         0|                  0|                 2|          0|                   0|                  0|                      0|                  0|                   0|                       0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    2|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|         0|\n",
       "|      03/03/2020|         1|                  0|                 7|          0|                   0|                  0|                      0|                  0|                   0|                       0|            0|                     0|                    1|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    3|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    1|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            1|                     0|                    2|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|         0|\n",
       "|      03/04/2020|         5|                  0|                 2|          0|                   0|                  0|                      0|                  0|                   0|                       0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            1|                     0|                    1|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            2|                     0|                    1|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            2|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|         0|\n",
       "+----------------+----------+-------------------+------------------+-----------+--------------------+-------------------+-----------------------+-------------------+--------------------+------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+----------+"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the covid dataset\n",
    "covid_df = spark.read.csv('../data/raw/virals/covid/cases-by-day.csv',\n",
    "    header = True)\n",
    "covid_df.limit(5)\n",
    "# TODO: commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum the number of incomplete datasets (ensure no incomplete values)\n",
    "if INTERMEDIATE_OUTPUTS:\n",
    "    sum(covid_df.select('INCOMPLETE'))\n",
    "# TODO: commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: commenting on covid cleaning\n",
    "COVID_KEEP_COLUMNS = {\n",
    "    'date_of_interest':'date'\n",
    "}\n",
    "\n",
    "COVID_CLEAN_COLUMNS = defaultdict(lambda: ch.non_negative)\n",
    "\n",
    "COVID_BOROUGHS = {\n",
    "    'BX_':'Bronx',\n",
    "    'BK_':'Brooklyn',\n",
    "    'MN_':'Manhattan',\n",
    "    'QN_':'Queens',\n",
    "    'SI_':'Staten Island',\n",
    "}\n",
    "\n",
    "COVID_COUNTS = {\n",
    "    'CASE_COUNT': 'cases', \n",
    "    'DEATH_COUNT': 'deaths', \n",
    "    'HOSPITALIZED_COUNT': 'hospitalised'\n",
    "}\n",
    "# TODO: commenting\n",
    "for prefix, new_prefix in COVID_BOROUGHS.items():\n",
    "    for suffix, new_suffix in COVID_COUNTS.items():\n",
    "        COVID_KEEP_COLUMNS[f'{prefix}{suffix}'] = f'{new_prefix}{new_suffix}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = ch.perform_cleaning(covid_df, mmwr_weeks_df, COVID_KEEP_COLUMNS, \n",
    "    COVID_CLEAN_COLUMNS)\n",
    "# TODO: commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "temp_df = None\n",
    "# TODO: commenting\n",
    "COVID_DATE_COLUMNS = [\n",
    "    F.col('date'), \n",
    "    F.col('week_ending'), \n",
    "    F.col('week_year'), \n",
    "    F.col('week_month'), \n",
    "    F.col('week_index'),\n",
    "    F.col('timeline')\n",
    "]\n",
    "\n",
    "# The data here is very wide, I'd rather just have a 'borough' column\n",
    "# for homogeneity of all the data\n",
    "for prefix in COVID_BOROUGHS.values():\n",
    "    borough_columns = []\n",
    "    for suffix in COVID_COUNTS.values():\n",
    "        borough_columns.append(F.col(f'{prefix}{suffix}').alias(suffix))\n",
    "\n",
    "    if temp_df == None:\n",
    "        temp_df = covid_df.select(COVID_DATE_COLUMNS + borough_columns)\\\n",
    "            .withColumn('borough', F.lit(prefix))\n",
    "    else:\n",
    "        temp_df = temp_df\\\n",
    "            .union(\n",
    "                covid_df.select(COVID_DATE_COLUMNS + borough_columns)\\\n",
    "                    .withColumn('borough', F.lit(prefix))\n",
    "            )\n",
    "    \n",
    "covid_df = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERMEDIATE_OUTPUTS:\n",
    "    covid_df.sort('week_index', 'date').limit(5)\n",
    "# TODO: commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned covid data\n",
    "# TODO: commenting\n",
    "covid_df.write.mode('overwrite').parquet('../data/curated/virals/covid/cleaned/cases-by-day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cleaning the flu dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Season</th><th>Region</th><th>County</th><th>CDC Week</th><th>Week Ending Date</th><th>Disease</th><th>Count</th><th>County Centroid</th><th>FIPS</th></tr>\n",
       "<tr><td>2012-2013</td><td>NYC</td><td>RICHMOND</td><td>10</td><td>03/09/2013</td><td>INFLUENZA_A</td><td>0</td><td>(40.5795, -74.1502)</td><td>36085</td></tr>\n",
       "<tr><td>2011-2012</td><td>CAPITAL DISTRICT</td><td>ALBANY</td><td>10</td><td>03/10/2012</td><td>INFLUENZA_UNSPECI...</td><td>0</td><td>(42.5882713, -73....</td><td>36001</td></tr>\n",
       "<tr><td>2009-2010</td><td>CAPITAL DISTRICT</td><td>SCHENECTADY</td><td>41</td><td>10/17/2009</td><td>INFLUENZA_UNSPECI...</td><td>0</td><td>(42.8175421, -74....</td><td>36093</td></tr>\n",
       "<tr><td>2010-2011</td><td>WESTERN</td><td>CHAUTAUQUA</td><td>19</td><td>05/14/2011</td><td>INFLUENZA_B</td><td>0</td><td>(42.3042159, -79....</td><td>36013</td></tr>\n",
       "<tr><td>2013-2014</td><td>METRO</td><td>DUTCHESS</td><td>44</td><td>11/02/2013</td><td>INFLUENZA_A</td><td>0</td><td>(41.7550085, -73....</td><td>36027</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+----------------+-----------+--------+----------------+--------------------+-----+--------------------+-----+\n",
       "|   Season|          Region|     County|CDC Week|Week Ending Date|             Disease|Count|     County Centroid| FIPS|\n",
       "+---------+----------------+-----------+--------+----------------+--------------------+-----+--------------------+-----+\n",
       "|2012-2013|             NYC|   RICHMOND|      10|      03/09/2013|         INFLUENZA_A|    0| (40.5795, -74.1502)|36085|\n",
       "|2011-2012|CAPITAL DISTRICT|     ALBANY|      10|      03/10/2012|INFLUENZA_UNSPECI...|    0|(42.5882713, -73....|36001|\n",
       "|2009-2010|CAPITAL DISTRICT|SCHENECTADY|      41|      10/17/2009|INFLUENZA_UNSPECI...|    0|(42.8175421, -74....|36093|\n",
       "|2010-2011|         WESTERN| CHAUTAUQUA|      19|      05/14/2011|         INFLUENZA_B|    0|(42.3042159, -79....|36013|\n",
       "|2013-2014|           METRO|   DUTCHESS|      44|      11/02/2013|         INFLUENZA_A|    0|(41.7550085, -73....|36027|\n",
       "+---------+----------------+-----------+--------+----------------+--------------------+-----+--------------------+-----+"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the flu dataset\n",
    "# TODO: commenting\n",
    "flu_df = spark.read.csv('../data/raw/virals/flu/cases-by-week.csv',\n",
    "    header=True)\n",
    "flu_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLU_KEEP_COLUMNS = {\n",
    "    'Week Ending Date': 'date',\n",
    "    'Region': 'region',\n",
    "    'County': 'borough',\n",
    "    'Disease': 'disease',\n",
    "    'Count': 'cases',\n",
    "}\n",
    "# TODO: commenting\n",
    "FLU_CLEAN_COLUMNS = {\n",
    "    'date': [],\n",
    "    'region': [lambda _: F.col('region') == 'NYC'],\n",
    "    'borough': [],\n",
    "    'disease': [],\n",
    "    'cases': [ch.non_negative]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: commenting\n",
    "flu_df:DataFrame = ch.perform_cleaning(flu_df, mmwr_weeks_df, FLU_KEEP_COLUMNS, \n",
    "    FLU_CLEAN_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERMEDIATE_OUTPUTS:\n",
    "    # get the list of distinct counties (column now called 'borough')\n",
    "    flu_df.select('borough').distinct().limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the boroughs to their proper names\n",
    "# from: https://portal.311.nyc.gov/article/?kanumber=KA-02877\n",
    "# also from map dict\n",
    "FLU_COUNTY_TO_BOROUGH = {\n",
    "    'BRONX': 'Bronx',\n",
    "    'KINGS': 'Brooklyn',\n",
    "    'NEW YORK': 'Manhattan',\n",
    "    'QUEENS': 'Queens',\n",
    "    'RICHMOND': 'Staten Island'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the mapping to the flu df\n",
    "flu_df = ch.replace_column_using_dict(flu_df, 'borough', FLU_COUNTY_TO_BOROUGH)\n",
    "\n",
    "# also remove the regions column (not needed anymore)\n",
    "columns_without_regions = flu_df.columns[:]\n",
    "columns_without_regions.remove('region')\n",
    "flu_df = flu_df.select(columns_without_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERMEDIATE_OUTPUTS:\n",
    "    flu_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned flu data\n",
    "flu_df.write.mode('overwrite').parquet('../data/curated/virals/flu/cleaned/cases-by-week')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
