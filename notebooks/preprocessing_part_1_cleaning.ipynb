{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAST30034: Applied Data Science Project 1\n",
    "---\n",
    "# Preprocessing Part 1: Cleaning The Data\n",
    "#### Xavier Travers (1178369)\n",
    "\n",
    "Cleaning the datasets of null, inconsistent, or unnecessary values.\n",
    "This is performed on the TLC data and COVID data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports used throughout this notebook\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import DataFrame, Column\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "import geopandas\n",
    "\n",
    "# add homemade helpers\n",
    "sys.path.insert(1, '../scripts')\n",
    "import helpers.cleaning_helpers as ch\n",
    "import helpers.join_helpers as jh\n",
    "\n",
    "# for printouts\n",
    "DEBUGGING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/12 10:27:06 WARN Utils: Your hostname, Ganymede resolves to a loopback address: 127.0.1.1; using 172.29.200.237 instead (on interface eth0)\n",
      "22/08/12 10:27:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/12 10:27:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName('MAST30034 XT Project 1')\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True) \n",
    "    .config('spark.sql.parquet.cacheMetadata', 'true')\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>year</th><th>month</th><th>day</th><th>cdc_week</th><th>week_index</th><th>us_format</th></tr>\n",
       "<tr><td>2018</td><td>12</td><td>30</td><td>1</td><td>1</td><td>12/30/2018</td></tr>\n",
       "<tr><td>2018</td><td>12</td><td>31</td><td>1</td><td>1</td><td>12/31/2018</td></tr>\n",
       "<tr><td>2019</td><td>1</td><td>1</td><td>1</td><td>1</td><td>01/01/2019</td></tr>\n",
       "<tr><td>2019</td><td>1</td><td>2</td><td>1</td><td>1</td><td>01/02/2019</td></tr>\n",
       "<tr><td>2019</td><td>1</td><td>3</td><td>1</td><td>1</td><td>01/03/2019</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+-----+---+--------+----------+----------+\n",
       "|year|month|day|cdc_week|week_index| us_format|\n",
       "+----+-----+---+--------+----------+----------+\n",
       "|2018|   12| 30|       1|         1|12/30/2018|\n",
       "|2018|   12| 31|       1|         1|12/31/2018|\n",
       "|2019|    1|  1|       1|         1|01/01/2019|\n",
       "|2019|    1|  2|       1|         1|01/02/2019|\n",
       "|2019|    1|  3|       1|         1|01/03/2019|\n",
       "+----+-----+---+--------+----------+----------+"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the cdc week file to convert all dates to cdc weeks now\n",
    "mmwr_weeks_df = spark.read.parquet('../data/raw/virals/mmwr_weeks.parquet')\n",
    "mmwr_weeks_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the zones dataset\n",
    "zones_df = spark.read.csv('../data/raw/tlc_zones/zones.csv',\n",
    "    header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cleaning the TLC dataset(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>VendorID</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>RatecodeID</th><th>store_and_fwd_flag</th><th>PULocationID</th><th>DOLocationID</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th><th>congestion_surcharge</th><th>airport_fee</th></tr>\n",
       "<tr><td>2</td><td>2019-07-01 00:51:04</td><td>2019-07-01 00:51:33</td><td>1.0</td><td>0.0</td><td>1.0</td><td>N</td><td>193</td><td>193</td><td>1</td><td>2.5</td><td>0.5</td><td>0.5</td><td>1.14</td><td>0.0</td><td>0.3</td><td>4.94</td><td>0.0</td><td>null</td></tr>\n",
       "<tr><td>2</td><td>2019-07-01 00:46:04</td><td>2019-07-01 01:05:46</td><td>1.0</td><td>4.16</td><td>1.0</td><td>N</td><td>234</td><td>25</td><td>2</td><td>16.5</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>20.3</td><td>2.5</td><td>null</td></tr>\n",
       "<tr><td>1</td><td>2019-07-01 00:25:09</td><td>2019-07-01 01:00:56</td><td>1.0</td><td>18.8</td><td>2.0</td><td>N</td><td>132</td><td>42</td><td>1</td><td>52.0</td><td>0.0</td><td>0.5</td><td>11.75</td><td>6.12</td><td>0.3</td><td>70.67</td><td>0.0</td><td>null</td></tr>\n",
       "<tr><td>2</td><td>2019-07-01 00:33:32</td><td>2019-07-01 01:15:27</td><td>1.0</td><td>18.46</td><td>2.0</td><td>N</td><td>132</td><td>142</td><td>1</td><td>52.0</td><td>0.0</td><td>0.5</td><td>11.06</td><td>0.0</td><td>0.3</td><td>66.36</td><td>2.5</td><td>null</td></tr>\n",
       "<tr><td>1</td><td>2019-07-01 00:00:55</td><td>2019-07-01 00:13:05</td><td>0.0</td><td>1.7</td><td>1.0</td><td>N</td><td>107</td><td>114</td><td>1</td><td>9.5</td><td>3.0</td><td>0.5</td><td>2.0</td><td>0.0</td><td>0.3</td><td>15.3</td><td>2.5</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
       "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
       "|       2| 2019-07-01 00:51:04|  2019-07-01 00:51:33|            1.0|          0.0|       1.0|                 N|         193|         193|           1|        2.5|  0.5|    0.5|      1.14|         0.0|                  0.3|        4.94|                 0.0|       null|\n",
       "|       2| 2019-07-01 00:46:04|  2019-07-01 01:05:46|            1.0|         4.16|       1.0|                 N|         234|          25|           2|       16.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        20.3|                 2.5|       null|\n",
       "|       1| 2019-07-01 00:25:09|  2019-07-01 01:00:56|            1.0|         18.8|       2.0|                 N|         132|          42|           1|       52.0|  0.0|    0.5|     11.75|        6.12|                  0.3|       70.67|                 0.0|       null|\n",
       "|       2| 2019-07-01 00:33:32|  2019-07-01 01:15:27|            1.0|        18.46|       2.0|                 N|         132|         142|           1|       52.0|  0.0|    0.5|     11.06|         0.0|                  0.3|       66.36|                 2.5|       null|\n",
       "|       1| 2019-07-01 00:00:55|  2019-07-01 00:13:05|            0.0|          1.7|       1.0|                 N|         107|         114|           1|        9.5|  3.0|    0.5|       2.0|         0.0|                  0.3|        15.3|                 2.5|       null|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_df = spark.read.parquet('../data/raw/tlc/yellow/2019-07.parquet/')\n",
    "example_df.limit(5)\n",
    "# TODO: commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>VendorID</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>RatecodeID</th><th>store_and_fwd_flag</th><th>PULocationID</th><th>DOLocationID</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th><th>congestion_surcharge</th><th>airport_fee</th></tr>\n",
       "<tr><td>2</td><td>2019-07-29 09:46:42</td><td>2019-07-29 15:12:31</td><td>1.0</td><td>311.56</td><td>4.0</td><td>N</td><td>68</td><td>265</td><td>2</td><td>1574.0</td><td>0.0</td><td>0.5</td><td>0.0</td><td>10.5</td><td>0.3</td><td>1587.8</td><td>2.5</td><td>null</td></tr>\n",
       "<tr><td>1</td><td>2019-07-17 13:42:23</td><td>2019-07-17 14:15:25</td><td>1.0</td><td>307.5</td><td>1.0</td><td>N</td><td>161</td><td>138</td><td>1</td><td>28.5</td><td>2.5</td><td>0.5</td><td>5.0</td><td>0.0</td><td>0.3</td><td>36.8</td><td>2.5</td><td>null</td></tr>\n",
       "<tr><td>2</td><td>2019-07-03 16:13:11</td><td>2019-07-03 20:09:21</td><td>2.0</td><td>180.09</td><td>5.0</td><td>N</td><td>93</td><td>265</td><td>1</td><td>400.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>57.12</td><td>0.3</td><td>457.42</td><td>0.0</td><td>null</td></tr>\n",
       "<tr><td>2</td><td>2019-07-19 07:01:46</td><td>2019-07-19 10:50:56</td><td>2.0</td><td>169.47</td><td>4.0</td><td>N</td><td>43</td><td>265</td><td>2</td><td>794.5</td><td>0.0</td><td>0.5</td><td>0.0</td><td>12.5</td><td>0.3</td><td>807.8</td><td>0.0</td><td>null</td></tr>\n",
       "<tr><td>2</td><td>2019-07-13 05:40:49</td><td>2019-07-13 08:32:15</td><td>4.0</td><td>168.44</td><td>4.0</td><td>N</td><td>132</td><td>265</td><td>2</td><td>796.5</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>797.8</td><td>0.0</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
       "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
       "|       2| 2019-07-29 09:46:42|  2019-07-29 15:12:31|            1.0|       311.56|       4.0|                 N|          68|         265|           2|     1574.0|  0.0|    0.5|       0.0|        10.5|                  0.3|      1587.8|                 2.5|       null|\n",
       "|       1| 2019-07-17 13:42:23|  2019-07-17 14:15:25|            1.0|        307.5|       1.0|                 N|         161|         138|           1|       28.5|  2.5|    0.5|       5.0|         0.0|                  0.3|        36.8|                 2.5|       null|\n",
       "|       2| 2019-07-03 16:13:11|  2019-07-03 20:09:21|            2.0|       180.09|       5.0|                 N|          93|         265|           1|      400.0|  0.0|    0.0|       0.0|       57.12|                  0.3|      457.42|                 0.0|       null|\n",
       "|       2| 2019-07-19 07:01:46|  2019-07-19 10:50:56|            2.0|       169.47|       4.0|                 N|          43|         265|           2|      794.5|  0.0|    0.5|       0.0|        12.5|                  0.3|       807.8|                 0.0|       null|\n",
       "|       2| 2019-07-13 05:40:49|  2019-07-13 08:32:15|            4.0|       168.44|       4.0|                 N|         132|         265|           2|      796.5|  0.5|    0.5|       0.0|         0.0|                  0.3|       797.8|                 0.0|       null|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_df.sort('trip_distance', ascending = False).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of the tlc datasets to clean \n",
    "# (I was originally planning on working on fhvhv and green as well)\n",
    "TLC_NAMES = ['yellow']\n",
    "\n",
    "# dictionary to rename all the columns I want to keep\n",
    "TLC_KEEP_COLUMNS = {\n",
    "    'tpep_pickup_datetime': 'date',\n",
    "    'passenger_count': 'passengers',\n",
    "    'trip_distance': 'trip_distance',\n",
    "    'PULocationID': 'pu_location_id',\n",
    "    'DOLocationID': 'do_location_id',\n",
    "    'hours_elapsed': 'hours_elapsed'\n",
    "    # #  below only apply to fhvhv\n",
    "    # 'hvfhs_license_num': 'fhvhv_license',\n",
    "    # 'pickup_datetime': 'date',\n",
    "    # 'trip_miles': 'trip_distance',\n",
    "    # 'shared_request_flag': 'shared'\n",
    "}\n",
    "\n",
    "# create a dictionary of the columns to keep and the required filters\n",
    "TLC_CLEAN_COLUMNS = {\n",
    "    'pu_location_id': [ch.non_null], \n",
    "    'do_location_id': [ch.non_null], \n",
    "    'passengers': [ch.non_null], \n",
    "    'trip_distance': [ch.non_null, ch.strictly_positive], \n",
    "    # 'fhvhv_license': [ch.non_null], \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the TLC names/types (~5-10 mins)\n",
    "# TODO: commenting\n",
    "# stacked_tlc_df = None\n",
    "# for name in TLC_NAMES:\n",
    "#     # iterate through the downloaded files per taxi type\n",
    "#     for filename in os.listdir(f'../data/raw/tlc/{name}'):\n",
    "\n",
    "#         # read the parquet in\n",
    "#         tlc_df = spark.read.parquet(f'../data/raw/tlc/{name}/{filename}')\n",
    "\n",
    "#         # debug info\n",
    "#         print(f'=== CLEANING \"{name}/{filename}\"')\n",
    "    \n",
    "#         if DEBUGGING:\n",
    "#             print(f'STARTING WITH {tlc_df.count()} ROWS')\n",
    "\n",
    "#         tlc_df = ch.perform_cleaning(tlc_df, mmwr_weeks_df, TLC_KEEP_COLUMNS, \n",
    "#             TLC_CLEAN_COLUMNS)\n",
    "\n",
    "#         if stacked_tlc_df == None:\n",
    "#             stacked_tlc_df = tlc_df\n",
    "#         else:\n",
    "#             stacked_tlc_df = stacked_tlc_df.union(tlc_df)\n",
    "\n",
    "#         if DEBUGGING:\n",
    "#             print(f'REDUCED TO {tlc_df.count()} ROWS')\n",
    "        \n",
    "        # # write to file system\n",
    "        # tlc_df.write.mode('overwrite')\\\n",
    "        #     .parquet(f'../data/curated/tlc/cleaned/{name}/{filename}')\n",
    "\n",
    "# stacked_tlc_rows = stacked_tlc_df.collect()\n",
    "# stacked_tlc_df = spark.createDataFrame(stacked_tlc_rows)\n",
    "\n",
    "# get the count of the elements\n",
    "# count_rows = stacked_tlc_df.count()\n",
    "# print(count_rows)\n",
    "\n",
    "# # remove the top and bottom 5% of values by trip distance (removes outliers)\n",
    "# stacked_tlc_df:DataFrame = stacked_tlc_df.sort('trip_distance')\n",
    "# stacked_tlc_df = stacked_tlc_df.limit(int(count_rows * 0.95))\n",
    "# stacked_tlc_df = stacked_tlc_df.sort('trip_distance', ascending = False)\n",
    "# stacked_tlc_df = stacked_tlc_df.limit(int(count_rows * 0.95))\n",
    "\n",
    "# # print(stacked_tlc_df.count())\n",
    "# stacked_tlc_df = stacked_tlc_df.sort('year', 'month')\n",
    "# stacked_tlc_df.write\\\n",
    "#     .partitionBy('year', 'month')\\\n",
    "#     .mode('overwrite')\\\n",
    "#     .parquet(f'../data/curated/tlc/cleaned/{name}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the tlc data\n",
    "tlc_df = jh.read_stacked_tlc_df(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>VendorID</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>RatecodeID</th><th>store_and_fwd_flag</th><th>PULocationID</th><th>DOLocationID</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th><th>congestion_surcharge</th><th>airport_fee</th></tr>\n",
       "<tr><td>1</td><td>2019-01-01 00:46:40</td><td>2019-01-01 00:53:20</td><td>1.0</td><td>1.5</td><td>1.0</td><td>N</td><td>151</td><td>239</td><td>1</td><td>7.0</td><td>0.5</td><td>0.5</td><td>1.65</td><td>0.0</td><td>0.3</td><td>9.95</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>1</td><td>2019-01-01 00:59:47</td><td>2019-01-01 01:18:59</td><td>1.0</td><td>2.6</td><td>1.0</td><td>N</td><td>239</td><td>246</td><td>1</td><td>14.0</td><td>0.5</td><td>0.5</td><td>1.0</td><td>0.0</td><td>0.3</td><td>16.3</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2</td><td>2018-12-21 13:48:30</td><td>2018-12-21 13:52:40</td><td>3.0</td><td>0.0</td><td>1.0</td><td>N</td><td>236</td><td>236</td><td>1</td><td>4.5</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>5.8</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2</td><td>2018-11-28 15:52:25</td><td>2018-11-28 15:55:45</td><td>5.0</td><td>0.0</td><td>1.0</td><td>N</td><td>193</td><td>193</td><td>2</td><td>3.5</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>7.55</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>2</td><td>2018-11-28 15:56:57</td><td>2018-11-28 15:58:33</td><td>5.0</td><td>0.0</td><td>2.0</td><td>N</td><td>193</td><td>193</td><td>2</td><td>52.0</td><td>0.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>55.55</td><td>null</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
       "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
       "|       1| 2019-01-01 00:46:40|  2019-01-01 00:53:20|            1.0|          1.5|       1.0|                 N|         151|         239|           1|        7.0|  0.5|    0.5|      1.65|         0.0|                  0.3|        9.95|                null|       null|\n",
       "|       1| 2019-01-01 00:59:47|  2019-01-01 01:18:59|            1.0|          2.6|       1.0|                 N|         239|         246|           1|       14.0|  0.5|    0.5|       1.0|         0.0|                  0.3|        16.3|                null|       null|\n",
       "|       2| 2018-12-21 13:48:30|  2018-12-21 13:52:40|            3.0|          0.0|       1.0|                 N|         236|         236|           1|        4.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         5.8|                null|       null|\n",
       "|       2| 2018-11-28 15:52:25|  2018-11-28 15:55:45|            5.0|          0.0|       1.0|                 N|         193|         193|           2|        3.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        7.55|                null|       null|\n",
       "|       2| 2018-11-28 15:56:57|  2018-11-28 15:58:33|            5.0|          0.0|       2.0|                 N|         193|         193|           2|       52.0|  0.0|    0.5|       0.0|         0.0|                  0.3|       55.55|                null|       null|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "tlc_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>VendorID</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>RatecodeID</th><th>store_and_fwd_flag</th><th>PULocationID</th><th>DOLocationID</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th><th>congestion_surcharge</th><th>airport_fee</th><th>elapsed_time</th><th>mph</th></tr>\n",
       "<tr><td>1</td><td>2019-01-01 00:46:40</td><td>2019-01-01 00:53:20</td><td>1.0</td><td>1.5</td><td>1.0</td><td>N</td><td>151</td><td>239</td><td>1</td><td>7.0</td><td>0.5</td><td>0.5</td><td>1.65</td><td>0.0</td><td>0.3</td><td>9.95</td><td>null</td><td>null</td><td>0.1111111111111111</td><td>13.5</td></tr>\n",
       "<tr><td>1</td><td>2019-01-01 00:59:47</td><td>2019-01-01 01:18:59</td><td>1.0</td><td>2.6</td><td>1.0</td><td>N</td><td>239</td><td>246</td><td>1</td><td>14.0</td><td>0.5</td><td>0.5</td><td>1.0</td><td>0.0</td><td>0.3</td><td>16.3</td><td>null</td><td>null</td><td>0.32</td><td>8.125</td></tr>\n",
       "<tr><td>2</td><td>2018-12-21 13:48:30</td><td>2018-12-21 13:52:40</td><td>3.0</td><td>0.0</td><td>1.0</td><td>N</td><td>236</td><td>236</td><td>1</td><td>4.5</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>5.8</td><td>null</td><td>null</td><td>0.06944444444444445</td><td>0.0</td></tr>\n",
       "<tr><td>2</td><td>2018-11-28 15:52:25</td><td>2018-11-28 15:55:45</td><td>5.0</td><td>0.0</td><td>1.0</td><td>N</td><td>193</td><td>193</td><td>2</td><td>3.5</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>7.55</td><td>null</td><td>null</td><td>0.05555555555555555</td><td>0.0</td></tr>\n",
       "<tr><td>2</td><td>2018-11-28 15:56:57</td><td>2018-11-28 15:58:33</td><td>5.0</td><td>0.0</td><td>2.0</td><td>N</td><td>193</td><td>193</td><td>2</td><td>52.0</td><td>0.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>55.55</td><td>null</td><td>null</td><td>0.026666666666666665</td><td>0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+--------------------+-----+\n",
       "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|        elapsed_time|  mph|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+--------------------+-----+\n",
       "|       1| 2019-01-01 00:46:40|  2019-01-01 00:53:20|            1.0|          1.5|       1.0|                 N|         151|         239|           1|        7.0|  0.5|    0.5|      1.65|         0.0|                  0.3|        9.95|                null|       null|  0.1111111111111111| 13.5|\n",
       "|       1| 2019-01-01 00:59:47|  2019-01-01 01:18:59|            1.0|          2.6|       1.0|                 N|         239|         246|           1|       14.0|  0.5|    0.5|       1.0|         0.0|                  0.3|        16.3|                null|       null|                0.32|8.125|\n",
       "|       2| 2018-12-21 13:48:30|  2018-12-21 13:52:40|            3.0|          0.0|       1.0|                 N|         236|         236|           1|        4.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         5.8|                null|       null| 0.06944444444444445|  0.0|\n",
       "|       2| 2018-11-28 15:52:25|  2018-11-28 15:55:45|            5.0|          0.0|       1.0|                 N|         193|         193|           2|        3.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        7.55|                null|       null| 0.05555555555555555|  0.0|\n",
       "|       2| 2018-11-28 15:56:57|  2018-11-28 15:58:33|            5.0|          0.0|       2.0|                 N|         193|         193|           2|       52.0|  0.0|    0.5|       0.0|         0.0|                  0.3|       55.55|                null|       null|0.026666666666666665|  0.0|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+--------------------+-----+"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# derive extra values which are used to filter out valid trips\n",
    "SECONDS_TO_HOURS = 1 / (60*60)\n",
    "tlc_df = tlc_df\\\n",
    "    .withColumn('hours_elapsed', \n",
    "        (\n",
    "            (F.col(\"tpep_dropoff_datetime\").cast(\"long\")\n",
    "            - F.col('tpep_pickup_datetime').cast(\"long\")) \n",
    "            * SECONDS_TO_HOURS\n",
    "        )\n",
    "    )\\\n",
    "    .withColumn('mph', (F.col('trip_distance') / F.col('elapsed_time')))\n",
    "tlc_df.limit(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ypdcrime.com/vt/article30.php?zoom_highlight=fifty+five+miles+per+hour#t1180-a.\n",
    "# As per: https://www.dot.ny.gov/divisions/operating/oom/transportation-systems/repository/TSMI-17-05.pdf\n",
    "# the NYS maximum speed limit is 65 mph. filter out trips faster than legal.\n",
    "tlc_df = tlc_df.where(\n",
    "    (F.col('mph').isNotNull()) &\n",
    "    (F.col('mph') <= 65)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>VendorID</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>RatecodeID</th><th>store_and_fwd_flag</th><th>PULocationID</th><th>DOLocationID</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th><th>congestion_surcharge</th><th>airport_fee</th><th>elapsed_time</th><th>mph</th></tr>\n",
       "<tr><td>1</td><td>2019-01-01 00:46:40</td><td>2019-01-01 00:53:20</td><td>1.0</td><td>1.5</td><td>1.0</td><td>N</td><td>151</td><td>239</td><td>1</td><td>7.0</td><td>0.5</td><td>0.5</td><td>1.65</td><td>0.0</td><td>0.3</td><td>9.95</td><td>null</td><td>null</td><td>0.1111111111111111</td><td>13.5</td></tr>\n",
       "<tr><td>1</td><td>2019-01-01 00:59:47</td><td>2019-01-01 01:18:59</td><td>1.0</td><td>2.6</td><td>1.0</td><td>N</td><td>239</td><td>246</td><td>1</td><td>14.0</td><td>0.5</td><td>0.5</td><td>1.0</td><td>0.0</td><td>0.3</td><td>16.3</td><td>null</td><td>null</td><td>0.32</td><td>8.125</td></tr>\n",
       "<tr><td>2</td><td>2018-12-21 13:48:30</td><td>2018-12-21 13:52:40</td><td>3.0</td><td>0.0</td><td>1.0</td><td>N</td><td>236</td><td>236</td><td>1</td><td>4.5</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>5.8</td><td>null</td><td>null</td><td>0.06944444444444445</td><td>0.0</td></tr>\n",
       "<tr><td>2</td><td>2018-11-28 15:52:25</td><td>2018-11-28 15:55:45</td><td>5.0</td><td>0.0</td><td>1.0</td><td>N</td><td>193</td><td>193</td><td>2</td><td>3.5</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>7.55</td><td>null</td><td>null</td><td>0.05555555555555555</td><td>0.0</td></tr>\n",
       "<tr><td>2</td><td>2018-11-28 15:56:57</td><td>2018-11-28 15:58:33</td><td>5.0</td><td>0.0</td><td>2.0</td><td>N</td><td>193</td><td>193</td><td>2</td><td>52.0</td><td>0.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>55.55</td><td>null</td><td>null</td><td>0.026666666666666665</td><td>0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+--------------------+-----+\n",
       "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|        elapsed_time|  mph|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+--------------------+-----+\n",
       "|       1| 2019-01-01 00:46:40|  2019-01-01 00:53:20|            1.0|          1.5|       1.0|                 N|         151|         239|           1|        7.0|  0.5|    0.5|      1.65|         0.0|                  0.3|        9.95|                null|       null|  0.1111111111111111| 13.5|\n",
       "|       1| 2019-01-01 00:59:47|  2019-01-01 01:18:59|            1.0|          2.6|       1.0|                 N|         239|         246|           1|       14.0|  0.5|    0.5|       1.0|         0.0|                  0.3|        16.3|                null|       null|                0.32|8.125|\n",
       "|       2| 2018-12-21 13:48:30|  2018-12-21 13:52:40|            3.0|          0.0|       1.0|                 N|         236|         236|           1|        4.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         5.8|                null|       null| 0.06944444444444445|  0.0|\n",
       "|       2| 2018-11-28 15:52:25|  2018-11-28 15:55:45|            5.0|          0.0|       1.0|                 N|         193|         193|           2|        3.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        7.55|                null|       null| 0.05555555555555555|  0.0|\n",
       "|       2| 2018-11-28 15:56:57|  2018-11-28 15:58:33|            5.0|          0.0|       2.0|                 N|         193|         193|           2|       52.0|  0.0|    0.5|       0.0|         0.0|                  0.3|       55.55|                null|       null|0.026666666666666665|  0.0|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+--------------------+-----+"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tlc_df.limit(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>VendorID</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>RatecodeID</th><th>store_and_fwd_flag</th><th>PULocationID</th><th>DOLocationID</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th><th>congestion_surcharge</th><th>airport_fee</th><th>elapsed_time</th><th>mph</th></tr>\n",
       "<tr><td>1</td><td>2020-06-02 06:36:15</td><td>2020-06-02 14:33:50</td><td>1.0</td><td>441.6</td><td>5.0</td><td>N</td><td>68</td><td>265</td><td>2</td><td>300.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.3</td><td>300.3</td><td>0.0</td><td>null</td><td>7.959722222222222</td><td>55.479322980282674</td></tr>\n",
       "<tr><td>1</td><td>2021-01-20 11:22:05</td><td>2021-01-20 19:47:56</td><td>1.0</td><td>427.7</td><td>1.0</td><td>Y</td><td>4</td><td>265</td><td>1</td><td>1128.5</td><td>2.5</td><td>0.5</td><td>1140.44</td><td>20.16</td><td>0.3</td><td>2292.4</td><td>2.5</td><td>null</td><td>8.430833333333334</td><td>50.73045369180586</td></tr>\n",
       "<tr><td>1</td><td>2020-07-30 15:10:02</td><td>2020-07-30 22:04:34</td><td>1.0</td><td>414.4</td><td>5.0</td><td>N</td><td>138</td><td>265</td><td>1</td><td>400.0</td><td>0.0</td><td>0.0</td><td>87.2</td><td>35.74</td><td>0.3</td><td>523.24</td><td>0.0</td><td>null</td><td>6.908888888888889</td><td>59.98070119009328</td></tr>\n",
       "<tr><td>2</td><td>2020-12-06 07:33:27</td><td>2020-12-06 14:01:29</td><td>1.0</td><td>407.78</td><td>5.0</td><td>N</td><td>264</td><td>265</td><td>1</td><td>200.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>11.75</td><td>0.3</td><td>215.55</td><td>2.5</td><td>null</td><td>6.467222222222222</td><td>63.05334593248002</td></tr>\n",
       "<tr><td>2</td><td>2019-12-25 20:44:07</td><td>2019-12-26 03:10:41</td><td>2.0</td><td>363.13</td><td>5.0</td><td>N</td><td>132</td><td>265</td><td>2</td><td>400.0</td><td>0.0</td><td>0.5</td><td>0.0</td><td>46.34</td><td>0.3</td><td>447.14</td><td>0.0</td><td>null</td><td>6.442777777777778</td><td>56.362335086660345</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-----------------+------------------+\n",
       "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|     elapsed_time|               mph|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-----------------+------------------+\n",
       "|       1| 2020-06-02 06:36:15|  2020-06-02 14:33:50|            1.0|        441.6|       5.0|                 N|          68|         265|           2|      300.0|  0.0|    0.0|       0.0|         0.0|                  0.3|       300.3|                 0.0|       null|7.959722222222222|55.479322980282674|\n",
       "|       1| 2021-01-20 11:22:05|  2021-01-20 19:47:56|            1.0|        427.7|       1.0|                 Y|           4|         265|           1|     1128.5|  2.5|    0.5|   1140.44|       20.16|                  0.3|      2292.4|                 2.5|       null|8.430833333333334| 50.73045369180586|\n",
       "|       1| 2020-07-30 15:10:02|  2020-07-30 22:04:34|            1.0|        414.4|       5.0|                 N|         138|         265|           1|      400.0|  0.0|    0.0|      87.2|       35.74|                  0.3|      523.24|                 0.0|       null|6.908888888888889| 59.98070119009328|\n",
       "|       2| 2020-12-06 07:33:27|  2020-12-06 14:01:29|            1.0|       407.78|       5.0|                 N|         264|         265|           1|      200.0|  0.0|    0.0|       1.0|       11.75|                  0.3|      215.55|                 2.5|       null|6.467222222222222| 63.05334593248002|\n",
       "|       2| 2019-12-25 20:44:07|  2019-12-26 03:10:41|            2.0|       363.13|       5.0|                 N|         132|         265|           2|      400.0|  0.0|    0.5|       0.0|       46.34|                  0.3|      447.14|                 0.0|       null|6.442777777777778|56.362335086660345|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-----------------+------------------+"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tlc_df.sort('trip_distance', ascending = False).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlc_df = ch.perform_cleaning(tlc_df, mmwr_weeks_df, TLC_KEEP_COLUMNS, \n",
    "    TLC_CLEAN_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>year</th><th>month</th><th>day</th><th>week_index</th><th>cdc_week</th><th>date</th><th>passengers</th><th>trip_distance</th><th>pu_location_id</th><th>do_location_id</th><th>pu_borough</th><th>do_borough</th></tr>\n",
       "<tr><td>2020</td><td>9</td><td>9</td><td>89</td><td>37</td><td>09/09/2020</td><td>1.0</td><td>358.33</td><td>186</td><td>69</td><td>Manhattan</td><td>Bronx</td></tr>\n",
       "<tr><td>2021</td><td>1</td><td>12</td><td>107</td><td>2</td><td>01/12/2021</td><td>0.0</td><td>326.1</td><td>234</td><td>39</td><td>Manhattan</td><td>Brooklyn</td></tr>\n",
       "<tr><td>2020</td><td>5</td><td>16</td><td>72</td><td>20</td><td>05/16/2020</td><td>2.0</td><td>305.1</td><td>4</td><td>79</td><td>Manhattan</td><td>Manhattan</td></tr>\n",
       "<tr><td>2020</td><td>11</td><td>13</td><td>98</td><td>46</td><td>11/13/2020</td><td>1.0</td><td>277.8</td><td>79</td><td>4</td><td>Manhattan</td><td>Manhattan</td></tr>\n",
       "<tr><td>2021</td><td>1</td><td>6</td><td>106</td><td>1</td><td>01/06/2021</td><td>1.0</td><td>271.4</td><td>229</td><td>39</td><td>Manhattan</td><td>Brooklyn</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+-----+---+----------+--------+----------+----------+-------------+--------------+--------------+----------+----------+\n",
       "|year|month|day|week_index|cdc_week|      date|passengers|trip_distance|pu_location_id|do_location_id|pu_borough|do_borough|\n",
       "+----+-----+---+----------+--------+----------+----------+-------------+--------------+--------------+----------+----------+\n",
       "|2020|    9|  9|        89|      37|09/09/2020|       1.0|       358.33|           186|            69| Manhattan|     Bronx|\n",
       "|2021|    1| 12|       107|       2|01/12/2021|       0.0|        326.1|           234|            39| Manhattan|  Brooklyn|\n",
       "|2020|    5| 16|        72|      20|05/16/2020|       2.0|        305.1|             4|            79| Manhattan| Manhattan|\n",
       "|2020|   11| 13|        98|      46|11/13/2020|       1.0|        277.8|            79|             4| Manhattan| Manhattan|\n",
       "|2021|    1|  6|       106|       1|01/06/2021|       1.0|        271.4|           229|            39| Manhattan|  Brooklyn|\n",
       "+----+-----+---+----------+--------+----------+----------+-------------+--------------+--------------+----------+----------+"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next, filter out trips which do not start and/or end within the 5 boroughs \n",
    "# the inner join should filter it all out (except for airports)\n",
    "tlc_df = ch.extract_borough_name(tlc_df, zones_df,  'pu')\n",
    "tlc_df = ch.extract_borough_name(tlc_df, zones_df,  'do')\n",
    "tlc_df.sort('trip_distance', ascending=False).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:===================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: 1.0, Q3: 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get the IQR of the trip distances\n",
    "q1, q3 = tlc_df.approxQuantile('trip_distance', [0.25, 0.75], 0.0001)\n",
    "print(f'Q1: {q1}, Q3: {q3}')\n",
    "iqr = q3 - q1\n",
    "IQR_MULTIPLIER = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:=====================================================>(155 + 1) / 156]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.07 32.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "q1, q2 = tlc_df\\\n",
    "    .where(F.col('pu_borough') == 'Staten Island')\\\n",
    "    .approxQuantile('trip_distance', [0.25, 0.75], 10 ** -5)\n",
    "print(q1, q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 358.33\n"
     ]
    }
   ],
   "source": [
    "q025, q975 = tlc_df.approxQuantile('trip_distance', [0.01, 0.99999], 10 ** -5)\n",
    "print(q025, q975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove the values that aren't within Q1 - 1.5 IQR and Q3 + 1.5 IQR\n",
    "# stacked_tlc_df = stacked_tlc_df.where(F.col('trip_distance').between(\n",
    "#     q1 - IQR_MULTIPLIER * iqr, q3 + IQR_MULTIPLIER * iqr\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "95613333"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tlc_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>year</th><th>month</th><th>day</th><th>week_index</th><th>cdc_week</th><th>date</th><th>passengers</th><th>trip_distance</th><th>pu_location_id</th><th>do_location_id</th><th>pu_borough</th><th>do_borough</th></tr>\n",
       "<tr><td>2020</td><td>9</td><td>9</td><td>89</td><td>37</td><td>09/09/2020</td><td>1.0</td><td>358.33</td><td>186</td><td>69</td><td>Manhattan</td><td>Bronx</td></tr>\n",
       "<tr><td>2021</td><td>1</td><td>12</td><td>107</td><td>2</td><td>01/12/2021</td><td>0.0</td><td>326.1</td><td>234</td><td>39</td><td>Manhattan</td><td>Brooklyn</td></tr>\n",
       "<tr><td>2020</td><td>5</td><td>16</td><td>72</td><td>20</td><td>05/16/2020</td><td>2.0</td><td>305.1</td><td>4</td><td>79</td><td>Manhattan</td><td>Manhattan</td></tr>\n",
       "<tr><td>2020</td><td>11</td><td>13</td><td>98</td><td>46</td><td>11/13/2020</td><td>1.0</td><td>277.8</td><td>79</td><td>4</td><td>Manhattan</td><td>Manhattan</td></tr>\n",
       "<tr><td>2021</td><td>1</td><td>6</td><td>106</td><td>1</td><td>01/06/2021</td><td>1.0</td><td>271.4</td><td>229</td><td>39</td><td>Manhattan</td><td>Brooklyn</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+-----+---+----------+--------+----------+----------+-------------+--------------+--------------+----------+----------+\n",
       "|year|month|day|week_index|cdc_week|      date|passengers|trip_distance|pu_location_id|do_location_id|pu_borough|do_borough|\n",
       "+----+-----+---+----------+--------+----------+----------+-------------+--------------+--------------+----------+----------+\n",
       "|2020|    9|  9|        89|      37|09/09/2020|       1.0|       358.33|           186|            69| Manhattan|     Bronx|\n",
       "|2021|    1| 12|       107|       2|01/12/2021|       0.0|        326.1|           234|            39| Manhattan|  Brooklyn|\n",
       "|2020|    5| 16|        72|      20|05/16/2020|       2.0|        305.1|             4|            79| Manhattan| Manhattan|\n",
       "|2020|   11| 13|        98|      46|11/13/2020|       1.0|        277.8|            79|             4| Manhattan| Manhattan|\n",
       "|2021|    1|  6|       106|       1|01/06/2021|       1.0|        271.4|           229|            39| Manhattan|  Brooklyn|\n",
       "+----+-----+---+----------+--------+----------+----------+-------------+--------------+--------------+----------+----------+"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tlc_df.sort('trip_distance', ascending=False).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save the stacked df by month\n",
    "tlc_df = tlc_df.sort('year', 'month')\n",
    "tlc_df.write\\\n",
    "    .partitionBy('year', 'month')\\\n",
    "    .mode('overwrite')\\\n",
    "    .parquet(f'../data/curated/tlc/cleaned/yellow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleaning the COVID dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/12 10:54:16 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>date_of_interest</th><th>CASE_COUNT</th><th>PROBABLE_CASE_COUNT</th><th>HOSPITALIZED_COUNT</th><th>DEATH_COUNT</th><th>PROBABLE_DEATH_COUNT</th><th>CASE_COUNT_7DAY_AVG</th><th>ALL_CASE_COUNT_7DAY_AVG</th><th>HOSP_COUNT_7DAY_AVG</th><th>DEATH_COUNT_7DAY_AVG</th><th>ALL_DEATH_COUNT_7DAY_AVG</th><th>BX_CASE_COUNT</th><th>BX_PROBABLE_CASE_COUNT</th><th>BX_HOSPITALIZED_COUNT</th><th>BX_DEATH_COUNT</th><th>BX_PROBABLE_DEATH_COUNT</th><th>BX_CASE_COUNT_7DAY_AVG</th><th>BX_PROBABLE_CASE_COUNT_7DAY_AVG</th><th>BX_ALL_CASE_COUNT_7DAY_AVG</th><th>BX_HOSPITALIZED_COUNT_7DAY_AVG</th><th>BX_DEATH_COUNT_7DAY_AVG</th><th>BX_ALL_DEATH_COUNT_7DAY_AVG</th><th>BK_CASE_COUNT</th><th>BK_PROBABLE_CASE_COUNT</th><th>BK_HOSPITALIZED_COUNT</th><th>BK_DEATH_COUNT</th><th>BK_PROBABLE_DEATH_COUNT</th><th>BK_CASE_COUNT_7DAY_AVG</th><th>BK_PROBABLE_CASE_COUNT_7DAY_AVG</th><th>BK_ALL_CASE_COUNT_7DAY_AVG</th><th>BK_HOSPITALIZED_COUNT_7DAY_AVG</th><th>BK_DEATH_COUNT_7DAY_AVG</th><th>BK_ALL_DEATH_COUNT_7DAY_AVG</th><th>MN_CASE_COUNT</th><th>MN_PROBABLE_CASE_COUNT</th><th>MN_HOSPITALIZED_COUNT</th><th>MN_DEATH_COUNT</th><th>MN_PROBABLE_DEATH_COUNT</th><th>MN_CASE_COUNT_7DAY_AVG</th><th>MN_PROBABLE_CASE_COUNT_7DAY_AVG</th><th>MN_ALL_CASE_COUNT_7DAY_AVG</th><th>MN_HOSPITALIZED_COUNT_7DAY_AVG</th><th>MN_DEATH_COUNT_7DAY_AVG</th><th>MN_ALL_DEATH_COUNT_7DAY_AVG</th><th>QN_CASE_COUNT</th><th>QN_PROBABLE_CASE_COUNT</th><th>QN_HOSPITALIZED_COUNT</th><th>QN_DEATH_COUNT</th><th>QN_PROBABLE_DEATH_COUNT</th><th>QN_CASE_COUNT_7DAY_AVG</th><th>QN_PROBABLE_CASE_COUNT_7DAY_AVG</th><th>QN_ALL_CASE_COUNT_7DAY_AVG</th><th>QN_HOSPITALIZED_COUNT_7DAY_AVG</th><th>QN_DEATH_COUNT_7DAY_AVG</th><th>QN_ALL_DEATH_COUNT_7DAY_AVG</th><th>SI_CASE_COUNT</th><th>SI_PROBABLE_CASE_COUNT</th><th>SI_HOSPITALIZED_COUNT</th><th>SI_DEATH_COUNT</th><th>SI_PROBABLE_DEATH_COUNT</th><th>SI_CASE_COUNT_7DAY_AVG</th><th>SI_PROBABLE_CASE_COUNT_7DAY_AVG</th><th>SI_ALL_CASE_COUNT_7DAY_AVG</th><th>SI_HOSPITALIZED_COUNT_7DAY_AVG</th><th>SI_DEATH_COUNT_7DAY_AVG</th><th>SI_ALL_DEATH_COUNT_7DAY_AVG</th><th>INCOMPLETE</th></tr>\n",
       "<tr><td>02/29/2020</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>03/01/2020</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>03/02/2020</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>03/03/2020</td><td>1</td><td>0</td><td>7</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>3</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>03/04/2020</td><td>5</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------------+----------+-------------------+------------------+-----------+--------------------+-------------------+-----------------------+-------------------+--------------------+------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+----------+\n",
       "|date_of_interest|CASE_COUNT|PROBABLE_CASE_COUNT|HOSPITALIZED_COUNT|DEATH_COUNT|PROBABLE_DEATH_COUNT|CASE_COUNT_7DAY_AVG|ALL_CASE_COUNT_7DAY_AVG|HOSP_COUNT_7DAY_AVG|DEATH_COUNT_7DAY_AVG|ALL_DEATH_COUNT_7DAY_AVG|BX_CASE_COUNT|BX_PROBABLE_CASE_COUNT|BX_HOSPITALIZED_COUNT|BX_DEATH_COUNT|BX_PROBABLE_DEATH_COUNT|BX_CASE_COUNT_7DAY_AVG|BX_PROBABLE_CASE_COUNT_7DAY_AVG|BX_ALL_CASE_COUNT_7DAY_AVG|BX_HOSPITALIZED_COUNT_7DAY_AVG|BX_DEATH_COUNT_7DAY_AVG|BX_ALL_DEATH_COUNT_7DAY_AVG|BK_CASE_COUNT|BK_PROBABLE_CASE_COUNT|BK_HOSPITALIZED_COUNT|BK_DEATH_COUNT|BK_PROBABLE_DEATH_COUNT|BK_CASE_COUNT_7DAY_AVG|BK_PROBABLE_CASE_COUNT_7DAY_AVG|BK_ALL_CASE_COUNT_7DAY_AVG|BK_HOSPITALIZED_COUNT_7DAY_AVG|BK_DEATH_COUNT_7DAY_AVG|BK_ALL_DEATH_COUNT_7DAY_AVG|MN_CASE_COUNT|MN_PROBABLE_CASE_COUNT|MN_HOSPITALIZED_COUNT|MN_DEATH_COUNT|MN_PROBABLE_DEATH_COUNT|MN_CASE_COUNT_7DAY_AVG|MN_PROBABLE_CASE_COUNT_7DAY_AVG|MN_ALL_CASE_COUNT_7DAY_AVG|MN_HOSPITALIZED_COUNT_7DAY_AVG|MN_DEATH_COUNT_7DAY_AVG|MN_ALL_DEATH_COUNT_7DAY_AVG|QN_CASE_COUNT|QN_PROBABLE_CASE_COUNT|QN_HOSPITALIZED_COUNT|QN_DEATH_COUNT|QN_PROBABLE_DEATH_COUNT|QN_CASE_COUNT_7DAY_AVG|QN_PROBABLE_CASE_COUNT_7DAY_AVG|QN_ALL_CASE_COUNT_7DAY_AVG|QN_HOSPITALIZED_COUNT_7DAY_AVG|QN_DEATH_COUNT_7DAY_AVG|QN_ALL_DEATH_COUNT_7DAY_AVG|SI_CASE_COUNT|SI_PROBABLE_CASE_COUNT|SI_HOSPITALIZED_COUNT|SI_DEATH_COUNT|SI_PROBABLE_DEATH_COUNT|SI_CASE_COUNT_7DAY_AVG|SI_PROBABLE_CASE_COUNT_7DAY_AVG|SI_ALL_CASE_COUNT_7DAY_AVG|SI_HOSPITALIZED_COUNT_7DAY_AVG|SI_DEATH_COUNT_7DAY_AVG|SI_ALL_DEATH_COUNT_7DAY_AVG|INCOMPLETE|\n",
       "+----------------+----------+-------------------+------------------+-----------+--------------------+-------------------+-----------------------+-------------------+--------------------+------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+----------+\n",
       "|      02/29/2020|         1|                  0|                 1|          0|                   0|                  0|                      0|                  0|                   0|                       0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    1|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            1|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|         0|\n",
       "|      03/01/2020|         0|                  0|                 1|          0|                   0|                  0|                      0|                  0|                   0|                       0|            0|                     0|                    1|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|         0|\n",
       "|      03/02/2020|         0|                  0|                 2|          0|                   0|                  0|                      0|                  0|                   0|                       0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    2|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|         0|\n",
       "|      03/03/2020|         1|                  0|                 7|          0|                   0|                  0|                      0|                  0|                   0|                       0|            0|                     0|                    1|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    3|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    1|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            1|                     0|                    2|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|         0|\n",
       "|      03/04/2020|         5|                  0|                 2|          0|                   0|                  0|                      0|                  0|                   0|                       0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            1|                     0|                    1|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            2|                     0|                    1|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            2|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|            0|                     0|                    0|             0|                      0|                     0|                              0|                         0|                             0|                      0|                          0|         0|\n",
       "+----------------+----------+-------------------+------------------+-----------+--------------------+-------------------+-----------------------+-------------------+--------------------+------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+-------------+----------------------+---------------------+--------------+-----------------------+----------------------+-------------------------------+--------------------------+------------------------------+-----------------------+---------------------------+----------+"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the covid dataset\n",
    "covid_df = spark.read.csv('../data/raw/virals/covid/cases-by-day.csv',\n",
    "    header = True)\n",
    "covid_df.limit(5)\n",
    "# TODO: commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'(INCOMPLETE + 0)'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum the number of incomplete datasets (ensure no incomplete values)\n",
    "sum(covid_df.select('INCOMPLETE'))\n",
    "# TODO: commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: commenting on covid cleaning\n",
    "COVID_KEEP_COLUMNS = {\n",
    "    'date_of_interest':'date'\n",
    "}\n",
    "\n",
    "COVID_CLEAN_COLUMNS = defaultdict(lambda: ch.non_negative)\n",
    "\n",
    "COVID_BOROUGHS = {\n",
    "    '': 'Overall',\n",
    "    'BX_':'Bronx',\n",
    "    'BK_':'Brooklyn',\n",
    "    'MN_':'Manhattan',\n",
    "    'QN_':'Queens',\n",
    "    'SI_':'Staten Island',\n",
    "}\n",
    "\n",
    "COVID_COUNTS = {\n",
    "    'CASE_COUNT': 'cases', \n",
    "    'DEATH_COUNT': 'deaths', \n",
    "    'HOSPITALIZED_COUNT': 'hospitalised'\n",
    "}\n",
    "# TODO: commenting\n",
    "for prefix, new_prefix in COVID_BOROUGHS.items():\n",
    "    for suffix, new_suffix in COVID_COUNTS.items():\n",
    "        COVID_KEEP_COLUMNS[f'{prefix}{suffix}'] = f'{new_prefix}{new_suffix}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = ch.perform_cleaning(covid_df, mmwr_weeks_df, COVID_KEEP_COLUMNS, \n",
    "    COVID_CLEAN_COLUMNS)\n",
    "# TODO: commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "temp_df = None\n",
    "# TODO: commenting\n",
    "COVID_DATE_COLUMNS = [\n",
    "    F.col('date'), F.col('year'), F.col('cdc_week'), F.col('week_index'),\n",
    "]\n",
    "\n",
    "# The data here is very wide, I'd rather just have a 'borough' column\n",
    "# for homogeneity of all the data\n",
    "for prefix in COVID_BOROUGHS.values():\n",
    "    borough_columns = []\n",
    "    for suffix in COVID_COUNTS.values():\n",
    "        borough_columns.append(F.col(f'{prefix}{suffix}').alias(suffix))\n",
    "\n",
    "    if temp_df == None:\n",
    "        temp_df = covid_df.select(COVID_DATE_COLUMNS + borough_columns)\\\n",
    "            .withColumn('borough', F.lit(prefix))\n",
    "    else:\n",
    "        temp_df = temp_df\\\n",
    "            .union(\n",
    "                covid_df.select(COVID_DATE_COLUMNS + borough_columns)\\\n",
    "                    .withColumn('borough', F.lit(prefix))\n",
    "            )\n",
    "    \n",
    "covid_df = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>date</th><th>year</th><th>cdc_week</th><th>week_index</th><th>cases</th><th>deaths</th><th>hospitalised</th><th>borough</th></tr>\n",
       "<tr><td>02/29/2020</td><td>2020</td><td>9</td><td>61</td><td>0</td><td>0</td><td>0</td><td>Bronx</td></tr>\n",
       "<tr><td>02/29/2020</td><td>2020</td><td>9</td><td>61</td><td>0</td><td>0</td><td>1</td><td>Brooklyn</td></tr>\n",
       "<tr><td>02/29/2020</td><td>2020</td><td>9</td><td>61</td><td>1</td><td>0</td><td>1</td><td>Overall</td></tr>\n",
       "<tr><td>02/29/2020</td><td>2020</td><td>9</td><td>61</td><td>1</td><td>0</td><td>0</td><td>Manhattan</td></tr>\n",
       "<tr><td>02/29/2020</td><td>2020</td><td>9</td><td>61</td><td>0</td><td>0</td><td>0</td><td>Queens</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+----+--------+----------+-----+------+------------+---------+\n",
       "|      date|year|cdc_week|week_index|cases|deaths|hospitalised|  borough|\n",
       "+----------+----+--------+----------+-----+------+------------+---------+\n",
       "|02/29/2020|2020|       9|        61|    0|     0|           0|    Bronx|\n",
       "|02/29/2020|2020|       9|        61|    0|     0|           1| Brooklyn|\n",
       "|02/29/2020|2020|       9|        61|    1|     0|           1|  Overall|\n",
       "|02/29/2020|2020|       9|        61|    1|     0|           0|Manhattan|\n",
       "|02/29/2020|2020|       9|        61|    0|     0|           0|   Queens|\n",
       "+----------+----+--------+----------+-----+------+------------+---------+"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_df.sort('week_index', 'date').limit(5)\n",
    "# TODO: commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned covid data\n",
    "# TODO: commenting\n",
    "covid_df.write.mode('overwrite').parquet('../data/curated/virals/covid/cases-by-day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cleaning the flu dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Season</th><th>Region</th><th>County</th><th>CDC Week</th><th>Week Ending Date</th><th>Disease</th><th>Count</th><th>County Centroid</th><th>FIPS</th></tr>\n",
       "<tr><td>2012-2013</td><td>NYC</td><td>RICHMOND</td><td>10</td><td>03/09/2013</td><td>INFLUENZA_A</td><td>0</td><td>(40.5795, -74.1502)</td><td>36085</td></tr>\n",
       "<tr><td>2011-2012</td><td>CAPITAL DISTRICT</td><td>ALBANY</td><td>10</td><td>03/10/2012</td><td>INFLUENZA_UNSPECI...</td><td>0</td><td>(42.5882713, -73....</td><td>36001</td></tr>\n",
       "<tr><td>2009-2010</td><td>CAPITAL DISTRICT</td><td>SCHENECTADY</td><td>41</td><td>10/17/2009</td><td>INFLUENZA_UNSPECI...</td><td>0</td><td>(42.8175421, -74....</td><td>36093</td></tr>\n",
       "<tr><td>2010-2011</td><td>WESTERN</td><td>CHAUTAUQUA</td><td>19</td><td>05/14/2011</td><td>INFLUENZA_B</td><td>0</td><td>(42.3042159, -79....</td><td>36013</td></tr>\n",
       "<tr><td>2013-2014</td><td>METRO</td><td>DUTCHESS</td><td>44</td><td>11/02/2013</td><td>INFLUENZA_A</td><td>0</td><td>(41.7550085, -73....</td><td>36027</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+----------------+-----------+--------+----------------+--------------------+-----+--------------------+-----+\n",
       "|   Season|          Region|     County|CDC Week|Week Ending Date|             Disease|Count|     County Centroid| FIPS|\n",
       "+---------+----------------+-----------+--------+----------------+--------------------+-----+--------------------+-----+\n",
       "|2012-2013|             NYC|   RICHMOND|      10|      03/09/2013|         INFLUENZA_A|    0| (40.5795, -74.1502)|36085|\n",
       "|2011-2012|CAPITAL DISTRICT|     ALBANY|      10|      03/10/2012|INFLUENZA_UNSPECI...|    0|(42.5882713, -73....|36001|\n",
       "|2009-2010|CAPITAL DISTRICT|SCHENECTADY|      41|      10/17/2009|INFLUENZA_UNSPECI...|    0|(42.8175421, -74....|36093|\n",
       "|2010-2011|         WESTERN| CHAUTAUQUA|      19|      05/14/2011|         INFLUENZA_B|    0|(42.3042159, -79....|36013|\n",
       "|2013-2014|           METRO|   DUTCHESS|      44|      11/02/2013|         INFLUENZA_A|    0|(41.7550085, -73....|36027|\n",
       "+---------+----------------+-----------+--------+----------------+--------------------+-----+--------------------+-----+"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the flu dataset\n",
    "# TODO: commenting\n",
    "flu_df = spark.read.csv('../data/raw/virals/flu/cases-by-week.csv',\n",
    "    header=True)\n",
    "flu_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLU_KEEP_COLUMNS = {\n",
    "    'Week Ending Date': 'date',\n",
    "    'Region': 'region',\n",
    "    'County': 'borough',\n",
    "    'Disease': 'disease',\n",
    "    'Count': 'cases',\n",
    "}\n",
    "# TODO: commenting\n",
    "FLU_CLEAN_COLUMNS = {\n",
    "    'date': [],\n",
    "    'region': [lambda _: F.col('region') == 'NYC'],\n",
    "    'borough': [],\n",
    "    'disease': [],\n",
    "    'cases': [ch.non_negative]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: commenting\n",
    "flu_df:DataFrame = ch.perform_cleaning(flu_df, mmwr_weeks_df, FLU_KEEP_COLUMNS, \n",
    "    FLU_CLEAN_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>borough</th></tr>\n",
       "<tr><td>BRONX</td></tr>\n",
       "<tr><td>RICHMOND</td></tr>\n",
       "<tr><td>NEW YORK</td></tr>\n",
       "<tr><td>KINGS</td></tr>\n",
       "<tr><td>QUEENS</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+\n",
       "| borough|\n",
       "+--------+\n",
       "|   BRONX|\n",
       "|RICHMOND|\n",
       "|NEW YORK|\n",
       "|   KINGS|\n",
       "|  QUEENS|\n",
       "+--------+"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the list of distinct counties (column now called 'borough')\n",
    "flu_df.select('borough').distinct().limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the boroughs to their proper names\n",
    "# from: https://portal.311.nyc.gov/article/?kanumber=KA-02877\n",
    "# also from map dict\n",
    "FLU_COUNTY_TO_BOROUGH = {\n",
    "    'BRONX': 'Bronx',\n",
    "    'KINGS': 'Brooklyn',\n",
    "    'NEW YORK': 'Manhattan',\n",
    "    'QUEENS': 'Queens',\n",
    "    'RICHMOND': 'Staten Island'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the mapping to the flu df\n",
    "flu_df = ch.replace_column_using_dict(flu_df, 'borough', FLU_COUNTY_TO_BOROUGH)\n",
    "\n",
    "# also remove the regions column (not needed anymore)\n",
    "columns_without_regions = flu_df.columns[:]\n",
    "columns_without_regions.remove('region')\n",
    "flu_df = flu_df.select(columns_without_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>year</th><th>month</th><th>day</th><th>week_index</th><th>cdc_week</th><th>date</th><th>borough</th><th>disease</th><th>cases</th></tr>\n",
       "<tr><td>2019</td><td>1</td><td>12</td><td>2</td><td>2</td><td>01/12/2019</td><td>Staten Island</td><td>INFLUENZA_UNSPECI...</td><td>0</td></tr>\n",
       "<tr><td>2019</td><td>10</td><td>19</td><td>42</td><td>42</td><td>10/19/2019</td><td>Manhattan</td><td>INFLUENZA_B</td><td>0</td></tr>\n",
       "<tr><td>2019</td><td>2</td><td>9</td><td>6</td><td>6</td><td>02/09/2019</td><td>Bronx</td><td>INFLUENZA_B</td><td>11</td></tr>\n",
       "<tr><td>2019</td><td>2</td><td>16</td><td>7</td><td>7</td><td>02/16/2019</td><td>Staten Island</td><td>INFLUENZA_UNSPECI...</td><td>0</td></tr>\n",
       "<tr><td>2021</td><td>12</td><td>25</td><td>156</td><td>51</td><td>12/25/2021</td><td>Queens</td><td>INFLUENZA_A</td><td>582</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+-----+---+----------+--------+----------+-------------+--------------------+-----+\n",
       "|year|month|day|week_index|cdc_week|      date|      borough|             disease|cases|\n",
       "+----+-----+---+----------+--------+----------+-------------+--------------------+-----+\n",
       "|2019|    1| 12|         2|       2|01/12/2019|Staten Island|INFLUENZA_UNSPECI...|    0|\n",
       "|2019|   10| 19|        42|      42|10/19/2019|    Manhattan|         INFLUENZA_B|    0|\n",
       "|2019|    2|  9|         6|       6|02/09/2019|        Bronx|         INFLUENZA_B|   11|\n",
       "|2019|    2| 16|         7|       7|02/16/2019|Staten Island|INFLUENZA_UNSPECI...|    0|\n",
       "|2021|   12| 25|       156|      51|12/25/2021|       Queens|         INFLUENZA_A|  582|\n",
       "+----+-----+---+----------+--------+----------+-------------+--------------------+-----+"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flu_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned flu data\n",
    "flu_df.write.mode('overwrite').parquet('../data/curated/virals/flu/cases-by-week')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
