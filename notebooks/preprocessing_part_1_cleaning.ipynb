{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAST30034: Applied Data Science Project 1\n",
    "---\n",
    "# Preprocessing Part 1: Cleaning The Data\n",
    "#### Xavier Travers (1178369)\n",
    "\n",
    "Cleaning the datasets of null, inconsistent, or unnecessary values.\n",
    "This is performed on the TLC data and COVID data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports used throughout this notebook\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import DataFrame, Column\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "import geopandas\n",
    "\n",
    "# add homemade helpers\n",
    "sys.path.insert(1, '../scripts')\n",
    "import helpers.cleaning_helpers as ch\n",
    "\n",
    "# for printouts\n",
    "DEBUGGING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/11 15:19:22 WARN Utils: Your hostname, Ganymede resolves to a loopback address: 127.0.1.1; using 172.29.200.206 instead (on interface eth0)\n",
      "22/08/11 15:19:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/11 15:19:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName('MAST30034 XT Project 1')\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True) \n",
    "    .config('spark.sql.parquet.cacheMetadata', 'true')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>year</th><th>month</th><th>day</th><th>cdc_week</th><th>week_index</th><th>us_format</th></tr>\n",
       "<tr><td>2017</td><td>12</td><td>31</td><td>1</td><td>1</td><td>12/31/2017</td></tr>\n",
       "<tr><td>2018</td><td>1</td><td>1</td><td>1</td><td>1</td><td>01/01/2018</td></tr>\n",
       "<tr><td>2018</td><td>1</td><td>2</td><td>1</td><td>1</td><td>01/02/2018</td></tr>\n",
       "<tr><td>2018</td><td>1</td><td>3</td><td>1</td><td>1</td><td>01/03/2018</td></tr>\n",
       "<tr><td>2018</td><td>1</td><td>4</td><td>1</td><td>1</td><td>01/04/2018</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+-----+---+--------+----------+----------+\n",
       "|year|month|day|cdc_week|week_index| us_format|\n",
       "+----+-----+---+--------+----------+----------+\n",
       "|2017|   12| 31|       1|         1|12/31/2017|\n",
       "|2018|    1|  1|       1|         1|01/01/2018|\n",
       "|2018|    1|  2|       1|         1|01/02/2018|\n",
       "|2018|    1|  3|       1|         1|01/03/2018|\n",
       "|2018|    1|  4|       1|         1|01/04/2018|\n",
       "+----+-----+---+--------+----------+----------+"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the cdc week file to convert all dates to cdc weeks now\n",
    "mmwr_weeks_df = spark.read.parquet('../data/raw/virals/mmwr_weeks.parquet')\n",
    "mmwr_weeks_df.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cleaning the TLC dataset(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>VendorID</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>RatecodeID</th><th>store_and_fwd_flag</th><th>PULocationID</th><th>DOLocationID</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th><th>congestion_surcharge</th><th>airport_fee</th></tr>\n",
       "<tr><td>2</td><td>2019-07-01 00:51:04</td><td>2019-07-01 00:51:33</td><td>1.0</td><td>0.0</td><td>1.0</td><td>N</td><td>193</td><td>193</td><td>1</td><td>2.5</td><td>0.5</td><td>0.5</td><td>1.14</td><td>0.0</td><td>0.3</td><td>4.94</td><td>0.0</td><td>null</td></tr>\n",
       "<tr><td>2</td><td>2019-07-01 00:46:04</td><td>2019-07-01 01:05:46</td><td>1.0</td><td>4.16</td><td>1.0</td><td>N</td><td>234</td><td>25</td><td>2</td><td>16.5</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>20.3</td><td>2.5</td><td>null</td></tr>\n",
       "<tr><td>1</td><td>2019-07-01 00:25:09</td><td>2019-07-01 01:00:56</td><td>1.0</td><td>18.8</td><td>2.0</td><td>N</td><td>132</td><td>42</td><td>1</td><td>52.0</td><td>0.0</td><td>0.5</td><td>11.75</td><td>6.12</td><td>0.3</td><td>70.67</td><td>0.0</td><td>null</td></tr>\n",
       "<tr><td>2</td><td>2019-07-01 00:33:32</td><td>2019-07-01 01:15:27</td><td>1.0</td><td>18.46</td><td>2.0</td><td>N</td><td>132</td><td>142</td><td>1</td><td>52.0</td><td>0.0</td><td>0.5</td><td>11.06</td><td>0.0</td><td>0.3</td><td>66.36</td><td>2.5</td><td>null</td></tr>\n",
       "<tr><td>1</td><td>2019-07-01 00:00:55</td><td>2019-07-01 00:13:05</td><td>0.0</td><td>1.7</td><td>1.0</td><td>N</td><td>107</td><td>114</td><td>1</td><td>9.5</td><td>3.0</td><td>0.5</td><td>2.0</td><td>0.0</td><td>0.3</td><td>15.3</td><td>2.5</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
       "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
       "|       2| 2019-07-01 00:51:04|  2019-07-01 00:51:33|            1.0|          0.0|       1.0|                 N|         193|         193|           1|        2.5|  0.5|    0.5|      1.14|         0.0|                  0.3|        4.94|                 0.0|       null|\n",
       "|       2| 2019-07-01 00:46:04|  2019-07-01 01:05:46|            1.0|         4.16|       1.0|                 N|         234|          25|           2|       16.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        20.3|                 2.5|       null|\n",
       "|       1| 2019-07-01 00:25:09|  2019-07-01 01:00:56|            1.0|         18.8|       2.0|                 N|         132|          42|           1|       52.0|  0.0|    0.5|     11.75|        6.12|                  0.3|       70.67|                 0.0|       null|\n",
       "|       2| 2019-07-01 00:33:32|  2019-07-01 01:15:27|            1.0|        18.46|       2.0|                 N|         132|         142|           1|       52.0|  0.0|    0.5|     11.06|         0.0|                  0.3|       66.36|                 2.5|       null|\n",
       "|       1| 2019-07-01 00:00:55|  2019-07-01 00:13:05|            0.0|          1.7|       1.0|                 N|         107|         114|           1|        9.5|  3.0|    0.5|       2.0|         0.0|                  0.3|        15.3|                 2.5|       null|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_df = spark.read.parquet('../data/raw/tlc/yellow/2019-07.parquet/')\n",
    "example_df.limit(5)\n",
    "# TODO: commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>VendorID</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>RatecodeID</th><th>store_and_fwd_flag</th><th>PULocationID</th><th>DOLocationID</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th><th>congestion_surcharge</th><th>airport_fee</th></tr>\n",
       "<tr><td>2</td><td>2019-07-29 09:46:42</td><td>2019-07-29 15:12:31</td><td>1.0</td><td>311.56</td><td>4.0</td><td>N</td><td>68</td><td>265</td><td>2</td><td>1574.0</td><td>0.0</td><td>0.5</td><td>0.0</td><td>10.5</td><td>0.3</td><td>1587.8</td><td>2.5</td><td>null</td></tr>\n",
       "<tr><td>1</td><td>2019-07-17 13:42:23</td><td>2019-07-17 14:15:25</td><td>1.0</td><td>307.5</td><td>1.0</td><td>N</td><td>161</td><td>138</td><td>1</td><td>28.5</td><td>2.5</td><td>0.5</td><td>5.0</td><td>0.0</td><td>0.3</td><td>36.8</td><td>2.5</td><td>null</td></tr>\n",
       "<tr><td>2</td><td>2019-07-03 16:13:11</td><td>2019-07-03 20:09:21</td><td>2.0</td><td>180.09</td><td>5.0</td><td>N</td><td>93</td><td>265</td><td>1</td><td>400.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>57.12</td><td>0.3</td><td>457.42</td><td>0.0</td><td>null</td></tr>\n",
       "<tr><td>2</td><td>2019-07-19 07:01:46</td><td>2019-07-19 10:50:56</td><td>2.0</td><td>169.47</td><td>4.0</td><td>N</td><td>43</td><td>265</td><td>2</td><td>794.5</td><td>0.0</td><td>0.5</td><td>0.0</td><td>12.5</td><td>0.3</td><td>807.8</td><td>0.0</td><td>null</td></tr>\n",
       "<tr><td>2</td><td>2019-07-13 05:40:49</td><td>2019-07-13 08:32:15</td><td>4.0</td><td>168.44</td><td>4.0</td><td>N</td><td>132</td><td>265</td><td>2</td><td>796.5</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>797.8</td><td>0.0</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
       "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
       "|       2| 2019-07-29 09:46:42|  2019-07-29 15:12:31|            1.0|       311.56|       4.0|                 N|          68|         265|           2|     1574.0|  0.0|    0.5|       0.0|        10.5|                  0.3|      1587.8|                 2.5|       null|\n",
       "|       1| 2019-07-17 13:42:23|  2019-07-17 14:15:25|            1.0|        307.5|       1.0|                 N|         161|         138|           1|       28.5|  2.5|    0.5|       5.0|         0.0|                  0.3|        36.8|                 2.5|       null|\n",
       "|       2| 2019-07-03 16:13:11|  2019-07-03 20:09:21|            2.0|       180.09|       5.0|                 N|          93|         265|           1|      400.0|  0.0|    0.0|       0.0|       57.12|                  0.3|      457.42|                 0.0|       null|\n",
       "|       2| 2019-07-19 07:01:46|  2019-07-19 10:50:56|            2.0|       169.47|       4.0|                 N|          43|         265|           2|      794.5|  0.0|    0.5|       0.0|        12.5|                  0.3|       807.8|                 0.0|       null|\n",
       "|       2| 2019-07-13 05:40:49|  2019-07-13 08:32:15|            4.0|       168.44|       4.0|                 N|         132|         265|           2|      796.5|  0.5|    0.5|       0.0|         0.0|                  0.3|       797.8|                 0.0|       null|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_df.sort('trip_distance', ascending = False).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of the tlc datasets to clean \n",
    "# (I was originally planning on working on fhvhv and green as well)\n",
    "TLC_NAMES = ['yellow']\n",
    "\n",
    "# dictionary to rename all the columns I want to keep\n",
    "TLC_KEEP_COLUMNS = {\n",
    "    'tpep_pickup_datetime': 'date',\n",
    "    'passenger_count': 'passengers',\n",
    "    'trip_distance': 'trip_distance',\n",
    "    'PULocationID': 'pu_location_id',\n",
    "    'DOLocationID': 'do_location_id',\n",
    "    # below only apply to fhvhv\n",
    "    # 'hvfhs_license_num': 'fhvhv_license',\n",
    "    # 'pickup_datetime': 'date',\n",
    "    # 'trip_miles': 'trip_distance',\n",
    "    # 'shared_request_flag': 'shared'\n",
    "}\n",
    "\n",
    "# create a dictionary of the columns to keep and the required filters\n",
    "TLC_CLEAN_COLUMNS = {\n",
    "    'pu_location_id': [ch.non_null], \n",
    "    'do_location_id': [ch.non_null], \n",
    "    'passengers': [ch.non_null], \n",
    "    'trip_distance': [ch.non_null, ch.strictly_positive], \n",
    "    # 'fhvhv_license': [ch.non_null], \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CLEANING \"yellow/2018-01.parquet\"\n",
      "=== CLEANING \"yellow/2018-02.parquet\"\n",
      "=== CLEANING \"yellow/2018-03.parquet\"\n",
      "=== CLEANING \"yellow/2018-04.parquet\"\n",
      "=== CLEANING \"yellow/2018-05.parquet\"\n",
      "=== CLEANING \"yellow/2018-06.parquet\"\n",
      "=== CLEANING \"yellow/2018-07.parquet\"\n",
      "=== CLEANING \"yellow/2018-08.parquet\"\n",
      "=== CLEANING \"yellow/2018-09.parquet\"\n",
      "=== CLEANING \"yellow/2018-10.parquet\"\n",
      "=== CLEANING \"yellow/2018-11.parquet\"\n",
      "=== CLEANING \"yellow/2018-12.parquet\"\n",
      "=== CLEANING \"yellow/2019-01.parquet\"\n",
      "=== CLEANING \"yellow/2019-02.parquet\"\n",
      "=== CLEANING \"yellow/2019-03.parquet\"\n",
      "=== CLEANING \"yellow/2019-04.parquet\"\n",
      "=== CLEANING \"yellow/2019-05.parquet\"\n",
      "=== CLEANING \"yellow/2019-06.parquet\"\n",
      "=== CLEANING \"yellow/2019-07.parquet\"\n",
      "=== CLEANING \"yellow/2019-08.parquet\"\n",
      "=== CLEANING \"yellow/2019-09.parquet\"\n",
      "=== CLEANING \"yellow/2019-10.parquet\"\n",
      "=== CLEANING \"yellow/2019-11.parquet\"\n",
      "=== CLEANING \"yellow/2019-12.parquet\"\n",
      "=== CLEANING \"yellow/2020-01.parquet\"\n",
      "=== CLEANING \"yellow/2020-02.parquet\"\n",
      "=== CLEANING \"yellow/2020-03.parquet\"\n",
      "=== CLEANING \"yellow/2020-04.parquet\"\n",
      "=== CLEANING \"yellow/2020-05.parquet\"\n",
      "=== CLEANING \"yellow/2020-06.parquet\"\n",
      "=== CLEANING \"yellow/2020-07.parquet\"\n",
      "=== CLEANING \"yellow/2020-08.parquet\"\n",
      "=== CLEANING \"yellow/2020-09.parquet\"\n",
      "=== CLEANING \"yellow/2020-10.parquet\"\n",
      "=== CLEANING \"yellow/2020-11.parquet\"\n",
      "=== CLEANING \"yellow/2020-12.parquet\"\n",
      "=== CLEANING \"yellow/2021-01.parquet\"\n",
      "=== CLEANING \"yellow/2021-02.parquet\"\n",
      "=== CLEANING \"yellow/2021-03.parquet\"\n",
      "=== CLEANING \"yellow/2021-04.parquet\"\n",
      "=== CLEANING \"yellow/2021-05.parquet\"\n",
      "=== CLEANING \"yellow/2021-06.parquet\"\n",
      "=== CLEANING \"yellow/2021-07.parquet\"\n",
      "=== CLEANING \"yellow/2021-08.parquet\"\n",
      "=== CLEANING \"yellow/2021-09.parquet\"\n",
      "=== CLEANING \"yellow/2021-10.parquet\"\n",
      "=== CLEANING \"yellow/2021-11.parquet\"\n",
      "=== CLEANING \"yellow/2021-12.parquet\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238084819\n",
      "22/08/11 15:22:58 WARN DAGScheduler: Broadcasting large task binary with size 1132.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:=====>                                                 (37 + 8) / 348]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/11 15:23:00 ERROR Executor: Exception in task 3.0 in stage 71.0 (TID 444)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:657)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$doExecute$4(limit.scala:231)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$Lambda$3066/2141549804.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$1524/83199108.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1485/416720070.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/08/11 15:23:00 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 3.0 in stage 71.0 (TID 444),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:657)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$doExecute$4(limit.scala:231)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$Lambda$3066/2141549804.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$1524/83199108.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1485/416720070.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/08/11 15:23:00 WARN TaskSetManager: Lost task 3.0 in stage 71.0 (TID 444) (172.29.200.206 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:657)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$doExecute$4(limit.scala:231)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$Lambda$3066/2141549804.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$1524/83199108.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1485/416720070.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "22/08/11 15:23:00 ERROR TaskSetManager: Task 3 in stage 71.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:======>                                                (39 + 8) / 348]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/11 15:23:01 ERROR Executor: Exception in task 11.0 in stage 71.0 (TID 452)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:657)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$doExecute$4(limit.scala:231)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$Lambda$3066/2141549804.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$1524/83199108.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1485/416720070.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/08/11 15:23:01 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 11.0 in stage 71.0 (TID 452),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:657)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$doExecute$4(limit.scala:231)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$Lambda$3066/2141549804.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$1524/83199108.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1485/416720070.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/08/11 15:23:01 ERROR FileFormatWriter: Aborting job b1fbe1d6-1127-4712-b2c5-61df651329f9.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 71.0 failed 1 times, most recent failure: Lost task 3.0 in stage 71.0 (TID 444) (172.29.200.206 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:657)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$doExecute$4(limit.scala:231)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$Lambda$3066/2141549804.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$1524/83199108.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1485/416720070.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:657)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$doExecute$4(limit.scala:231)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$Lambda$3066/2141549804.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$1524/83199108.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1485/416720070.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/digitaldata/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/digitaldata/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/digitaldata/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/digitaldata/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/digitaldata/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/digitaldata/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o5052.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/digit/Desktop/Classes/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_1_cleaning.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/digit/Desktop/Classes/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_1_cleaning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# print(stacked_tlc_df.count())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/digit/Desktop/Classes/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_1_cleaning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m stacked_tlc_df \u001b[39m=\u001b[39m stacked_tlc_df\u001b[39m.\u001b[39msort(\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmonth\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/digit/Desktop/Classes/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_1_cleaning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m stacked_tlc_df\u001b[39m.\u001b[39;49mwrite\\\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/digit/Desktop/Classes/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_1_cleaning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39m.\u001b[39;49mpartitionBy(\u001b[39m'\u001b[39;49m\u001b[39myear\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mmonth\u001b[39;49m\u001b[39m'\u001b[39;49m)\\\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/digit/Desktop/Classes/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_1_cleaning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39m.\u001b[39;49mmode(\u001b[39m'\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m'\u001b[39;49m)\\\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/digit/Desktop/Classes/ADS/mast30034-project-1-DigitalData/notebooks/preprocessing_part_1_cleaning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39m.\u001b[39;49mparquet(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m../data/curated/tlc/cleaned/\u001b[39;49m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m.parquet\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py:1140\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[0;32m-> 1140\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m answer[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o5052.parquet"
     ]
    }
   ],
   "source": [
    "# iterate through the TLC names/types (~5-10 mins)\n",
    "# TODO: commenting\n",
    "stacked_tlc_df = None\n",
    "for name in TLC_NAMES:\n",
    "    # iterate through the downloaded files per taxi type\n",
    "    for filename in os.listdir(f'../data/raw/tlc/{name}'):\n",
    "\n",
    "        # read the parquet in\n",
    "        tlc_df = spark.read.parquet(f'../data/raw/tlc/{name}/{filename}')\n",
    "\n",
    "        # debug info\n",
    "        print(f'=== CLEANING \"{name}/{filename}\"')\n",
    "    \n",
    "        if DEBUGGING:\n",
    "            print(f'STARTING WITH {tlc_df.count()} ROWS')\n",
    "\n",
    "        tlc_df = ch.perform_cleaning(tlc_df, mmwr_weeks_df, TLC_KEEP_COLUMNS, \n",
    "            TLC_CLEAN_COLUMNS)\n",
    "\n",
    "        if stacked_tlc_df == None:\n",
    "            stacked_tlc_df = tlc_df\n",
    "        else:\n",
    "            stacked_tlc_df = stacked_tlc_df.union(tlc_df)\n",
    "\n",
    "        if DEBUGGING:\n",
    "            print(f'REDUCED TO {tlc_df.count()} ROWS')\n",
    "        \n",
    "        # write to file system\n",
    "        # tlc_df.write.mode('overwrite')\\\n",
    "        #     .parquet(f'../data/curated/tlc/cleaned/{name}/{filename}')\n",
    "\n",
    "\n",
    "# get the count of the elements\n",
    "count_rows = stacked_tlc_df.count()\n",
    "print(count_rows)\n",
    "\n",
    "# remove the top and bottom 5% of values by trip distance (removes outliers)\n",
    "stacked_tlc_df:DataFrame = stacked_tlc_df.sort('trip_distance')\n",
    "stacked_tlc_df = stacked_tlc_df.limit(int(count_rows * 0.95))\n",
    "stacked_tlc_df = stacked_tlc_df.sort('trip_distance', ascending = False)\n",
    "stacked_tlc_df = stacked_tlc_df.limit(int(count_rows * 0.95))\n",
    "\n",
    "# print(stacked_tlc_df.count())\n",
    "stacked_tlc_df = stacked_tlc_df.sort('year', 'month')\n",
    "stacked_tlc_df.write\\\n",
    "    .partitionBy('year', 'month')\\\n",
    "    .mode('overwrite')\\\n",
    "    .parquet(f'../data/curated/tlc/cleaned/{name}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleaning the COVID dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the covid dataset\n",
    "covid_df = spark.read.csv('../data/raw/virals/covid/cases-by-day.csv',\n",
    "    header = True)\n",
    "covid_df.limit(5)\n",
    "# TODO: commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum the number of incomplete datasets (ensure no incomplete values)\n",
    "sum(covid_df.select('INCOMPLETE'))\n",
    "# TODO: commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: commenting on covid cleaning\n",
    "COVID_KEEP_COLUMNS = {\n",
    "    'date_of_interest':'date'\n",
    "}\n",
    "\n",
    "COVID_CLEAN_COLUMNS = defaultdict(lambda: ch.non_negative)\n",
    "\n",
    "COVID_BOROUGHS = {\n",
    "    '': 'Overall',\n",
    "    'BX_':'Bronx',\n",
    "    'BK_':'Brooklyn',\n",
    "    'MN_':'Manhattan',\n",
    "    'QN_':'Queens',\n",
    "    'SI_':'Staten Island',\n",
    "}\n",
    "\n",
    "COVID_COUNTS = {\n",
    "    'CASE_COUNT': 'cases', \n",
    "    'DEATH_COUNT': 'deaths', \n",
    "    'HOSPITALIZED_COUNT': 'hospitalised'\n",
    "}\n",
    "# TODO: commenting\n",
    "for prefix, new_prefix in COVID_BOROUGHS.items():\n",
    "    for suffix, new_suffix in COVID_COUNTS.items():\n",
    "        COVID_KEEP_COLUMNS[f'{prefix}{suffix}'] = f'{new_prefix}{new_suffix}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = ch.perform_cleaning(covid_df, mmwr_weeks_df, COVID_KEEP_COLUMNS, \n",
    "    COVID_CLEAN_COLUMNS)\n",
    "# TODO: commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "temp_df = None\n",
    "# TODO: commenting\n",
    "COVID_DATE_COLUMNS = [\n",
    "    F.col('date'), F.col('year'), F.col('cdc_week'), F.col('week_index'),\n",
    "]\n",
    "\n",
    "# The data here is very wide, I'd rather just have a 'borough' column\n",
    "# for homogeneity of all the data\n",
    "for prefix in COVID_BOROUGHS.values():\n",
    "    borough_columns = []\n",
    "    for suffix in COVID_COUNTS.values():\n",
    "        borough_columns.append(F.col(f'{prefix}{suffix}').alias(suffix))\n",
    "\n",
    "    if temp_df == None:\n",
    "        temp_df = covid_df.select(COVID_DATE_COLUMNS + borough_columns)\\\n",
    "            .withColumn('borough', F.lit(prefix))\n",
    "    else:\n",
    "        temp_df = temp_df\\\n",
    "            .union(\n",
    "                covid_df.select(COVID_DATE_COLUMNS + borough_columns)\\\n",
    "                    .withColumn('borough', F.lit(prefix))\n",
    "            )\n",
    "    \n",
    "covid_df = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df.sort('week_index', 'date').limit(5)\n",
    "# TODO: commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned covid data\n",
    "# TODO: commenting\n",
    "covid_df.write.mode('overwrite').parquet('../data/curated/virals/covid/cases-by-day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cleaning the flu dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the flu dataset\n",
    "# TODO: commenting\n",
    "flu_df = spark.read.csv('../data/raw/virals/flu/cases-by-week.csv',\n",
    "    header=True)\n",
    "flu_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLU_KEEP_COLUMNS = {\n",
    "    'Week Ending Date': 'date',\n",
    "    'Region': 'region',\n",
    "    'County': 'borough',\n",
    "    'Disease': 'disease',\n",
    "    'Count': 'cases',\n",
    "}\n",
    "# TODO: commenting\n",
    "FLU_CLEAN_COLUMNS = {\n",
    "    'date': [],\n",
    "    'region': [lambda _: F.col('region') == 'NYC'],\n",
    "    'borough': [],\n",
    "    'disease': [],\n",
    "    'cases': [ch.non_negative]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: commenting\n",
    "flu_df:DataFrame = ch.perform_cleaning(flu_df, mmwr_weeks_df, FLU_KEEP_COLUMNS, \n",
    "    FLU_CLEAN_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of distinct counties (column now called 'borough')\n",
    "flu_df.select('borough').distinct().limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the boroughs to their proper names\n",
    "# from: https://portal.311.nyc.gov/article/?kanumber=KA-02877\n",
    "# also from map dict\n",
    "FLU_COUNTY_TO_BOROUGH = {\n",
    "    'BRONX': 'Bronx',\n",
    "    'KINGS': 'Brooklyn',\n",
    "    'NEW YORK': 'Manhattan',\n",
    "    'QUEENS': 'Queens',\n",
    "    'RICHMOND': 'Staten Island'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the mapping to the flu df\n",
    "flu_df = ch.replace_column_using_dict(flu_df, 'borough', FLU_COUNTY_TO_BOROUGH)\n",
    "\n",
    "# also remove the regions column (not needed anymore)\n",
    "columns_without_regions = flu_df.columns[:]\n",
    "columns_without_regions.remove('region')\n",
    "flu_df = flu_df.select(columns_without_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flu_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned flu data\n",
    "flu_df.write.mode('overwrite').parquet('../data/curated/virals/flu/cases-by-week')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
